['To stay competitive global advertising holding companies have been investing in their data', 'Chief Business Officer at Cognyte, head of Cognyte’s product, global R&D, business units, and go-to-market strategy.\nGETTY\nOur world has become increasingly digital. The pandemic that drove the world indoors has only accelerated this. In fact, experts believe that 463 exabytes of data will be created daily worldwide by 2025. Brands and organizations are incorporating this data into their business models — some more successfully than others. Precise big data fusion and analytics are dramatically altering the way organizations function. In 2021, we must “work smarter, not harder.”\nHow? Paired with analytics, big data fusion allows companies to take data drawn from multiple sources to create and build a more sophisticated and cohesive model and understand the data better. Although organizations may have already started to use big data fusion, some may be behind the curve.\nPROMOTED\nTo succeed, organizations must also invest in big data fusion technologies, artificial intelligence (AI) and machine learning algorithms. AI will allow these companies to sift through data from different sources to create cohesive and accurate insights. The rise of AI in recent years has made big data fusion methodologies and tools increasingly common. But are companies leveraging this enough?\nThe Foundation Of Big Data Fusion Analytics here must be a foundation laid out for analytics before big data fusion is able to properly function and be useful to organizations. Like other technologies, it’s critical that leaders understand their goals before investing in this technology. Big data fusion is a good solution for organizations that are inundated with data but have no way to filter or analyze it — or to extract meaningful insights.\nMORE FOR YOU\nGoogle Issues Warning For 2 Billion Chrome Users\nForget The MacBook Pro, Apple Has Bigger Plans\nGoogle Discounts Pixel 6, Nest & Pixel Buds In Limited-Time Sale Event\nHowever, before an organization can invest in and utilize big data technologies to their full potential, it must have the foundation — AI algorithms and analytics — in place to check for anomalies, picking up on different patterns of behaviors. So, although big data fusion alone can find deeper meaning in the different data sources, organizations will be left with no actionable next steps without analytics. With the combination of both types of technology, organizations can benefit from true insights.\nIn the security domain, big data fusion analytics gives organizations the information to head off potential wrongdoers before they’ve had the opportunity to cause harm. Big data security and investigative analytics can reduce massive flows of raw events to a controllable number of concise and clearly organized amounts to guide decisions. And data fusion will continue to work, keeping all historical information available for analysis and providing experts with enough information to guide decisions (possibly connecting it to a previous anomaly or preparing for a potential incident).\nForbes Innovation\nNutanix Rajiv Ramaswami On\nHis First Year As CEO\nThe Big Data Behind The Fusion Analytics Whenever big data is discussed, it’s usually followed up by the age-old debate over quality or quantity. The amount of data has increased exponentially over the past years, which is good when the right tools are in place. For example, AI and machine learning algorithms can collect data, cleanse, index and fuse it, then process it into insights.\nWith the right big data platforms in place, the more data, the better — as long as organizations have the capabilities to properly analyze it. Organizations are drowning in data, with around 2.5 quintillion bytes of new data created per day. It makes sense: Data has quickly (and effectively) become what organizations in the security space use for early warning and detecting fraud. Put simply, data may be the answer to most questions organizations are trying to answer.\nBut investigative organizations are often unable to make use of their data due to the massive volumes, the wide variety of sources and the siloed nature of data storage. And untapped, unmined data is useless when it comes to detecting and preventing threats. Why miss out? In the security domain, big data analytics can identify outliers and other anomalies, which almost always indicate suspicious or malicious activity.\nHow Companies Are Using Fusion Analytics In one national security organization, investigations were taking too long (months or even years), and often did not reach a conclusion or provide actionable outcomes. The head of investigations understood that the tools the investigative teams were using could no longer fit their purpose, and they looked at a combination of big data fusion and analytics to meet their needs.\nIn another example, a large tech manufacturer with thousands of employees worldwide experienced several security breaches. They needed to ensure that only authorized personnel gain access to restricted areas to protect their intellectual property, assets and employees. By shifting from standard video- and security-camera monitoring to an integrated and analytics-driven approach, they could fuse data from multiple sources, analyze it and generate new and actionable insights about threats they were previously unaware of.\nThings To Keep In MindWhen big data fusion is paired with good analytics, organizations can learn more, faster and derive actionable insights. But it’s important to keep several things in mind.\nBig data analytics can be used to either parse through all data and potentially include false positives or be fine-tuned to be more precise in what data it reviews. In the latter, there will be fewer results, but the insights may be more precise. For a security organization, for instance, that may mean that some potential cases of data won’t be analyzed, but the organization will be able to locate malicious activity (with fewer false alarms) more precisely.\nBig data fusion isn’t a “set it and forget it” solution. It will only be as helpful as the analytics and algorithms that derive actionable intelligence. Ultimately, although big data may be useful to an organization, it’s also highly recommended that the organization have a clear plan for what happens to the data afterward. Harvesting the data isn’t the issue. If the organization isn’t ready to understand what it means, it could be left with a mountain of data from different sources and no clue where to begin. It’s up to organizations to ensure they have “data enlightenment” — in other words, they connect the dots — to understand the story the data are telling and what their next steps should be.\nForbes Technology Council is an invitation-only community for world-class CIOs, CTOs and technology executives. Do I qualify? ', 'Founder of ktc, I have a passion for service design, customer experience and entrepreneurship.\nGETTY\nYou might remember this story from 2012 that underlines the power of big data: Based on the buying behavior of a teenager, U.S. department store Target concluded that she was pregnant and sent her an ad for baby-related products. Her parents angrily called the manager of the branch concerned. A few days later, however, the parents\' apology came to the manager. It turned out that Target\'s data system knew their daughter better than they did themselves.\nAnd indeed, big data has proven its value for many things related to sales and marketing, including for the personalization of promotions, recommendations and personalization of products. Big data creates unprecedented opportunities to quickly deduce "what" is happening from very large numbers of flat data and to make statements and/or predictions based on that. Due to an increasing belief in technology, data and machine learning, it seems attractive to also use big data for innovation.\nPROMOTED\nBut that\'s when it often goes wrong. While big data can tell a lot about "what" is happening, it has big shortcomings in telling you "why" things are happening. And that latter question is exactly the one fundamental for innovation. \nAn example is the initially failed product Febreze from P&G. First introduced to test markets in the early 1990s, P&G used TV commercials to teach consumers what smells the product could eliminate and what it could be used on, such as fabrics, furniture and other household items. The company thought it had a winning product, but as months went by, sales dropped. Its leaders couldn\'t understand why until they talked to customers, who showed them that it was hard to market a product that neutralizes odors to consumers who don\'t believe odors exist in their homes. This shifted Febreze\'s marketing and the work its innovation team was doing. Now Febreze is known for its refreshing scents that act almost as a "reward" after cleaning. \nMORE FOR YOU\n‘We Can Control Our Own Destiny’: John Zimmer Shares Lyft’s Vision For The Company’s Future And $1 Trillion Market Opportunity\nFour New Microsoft Surface Computers Plus A Folding Phone—And Other Small Business Tech News\nThe LSE Alumni Turning Their University Into A Startup Powerhouse\nThis product innovation and launch is just one of many examples where a different type of data is needed, providing insight into deep-seated human aspirations and needs. \nInvest in thick data to understand the "why" rather than the "what." \nWe call the beforementioned data "thick data." It concerns small but very well-picked samples, data collection in the user context and a focus on deeper insights. With thick data, it\'s about people, feelings, situations, behavior and the rich data that results from research on these aspects. Instead of relying on data analysis in large numbers, insight is gathered by observing people and doing in-depth interviews in which not the "what" but the "why" question is central — and in which the surprise is central by asking further questions.\nForbes Small Business\nHow Entrepreneurs Can\nLeverage Visualization: A\nNeuroscientist Explains\nIn my own practice, I use an extensive set of techniques that involve thick data, each with its pros and cons and context in which they are best suited. Let me share some examples:\nJob-to-be-done research exposes why a user buys a certain product so that you gain insight into what improvement means for the user. Doing this type of research, a fast-food company might found out that it should not improve or add taste options for its milkshakes but innovate on long-lasting taste, compatibility with cup holders and a straw that lasts.\nUnmet need-finding exposes which problems and priorities customers have in addition to the ones solved by your product or service. Thick data provides direction for opportunities to broaden or create completely new services. This is why Netflix makes content now instead of only being a streaming provider. \nCustomer journey/experience mapping exposes what feelings customers have when consuming your services. This thick data provides direction in opportunities for better customer experiences. At a large airport in the Netherlands, my company discovered that the experience was affected by a hostile security experience that we were able to revert into a hospitable experience. \nPretotyping exposes whether (and which) customers understand your product idea and how they perceive the value by displaying a fake version of it in stores or digitally.\nThick data gives companies the ability to deduce "why" things happen from small numbers of rich data. Finding relevant pointers for innovation often requires this type of research and mindset.\nDo companies have to forget about big data in order to innovate?\nNot at all. The strength lies in knowing when to apply both disciplines and how they can reinforce each other. For example, one can use thick data to provide starting points for new customer problems or needs, which can then be validated and deepened via the support of big data available in systems. Or vice versa: Through constant NPS measurements, one can see trends in customer experience or certain moments when customer experience drops and then find out through interviews what caused them, making designing relevant interventions easier.\nForbes Business Council is the foremost growth and networking organization for business owners and leaders. Do I qualify?', 'BRAZIL - 2020/08/26: In this photo illustration the Palantir Technologies logo seen displayed on a ... [+] SOPA IMAGES/LIGHTROCKET VIA GETTY IMAGES\nBig data and analytics player Palantir stock (NYSE: PLTR) has declined by about 30% over the last month, considerably underperforming the S&P 500 which remains down by about 2.5% over the same period. There have been a couple of factors driving the sell-off. Firstly, although Palantir posted stronger than expected Q3 results in early November, the stock fell post-earnings as investors were apparently concerned by the lower growth rate for the company’s government business versus prior quarters, and limited traction in its commercial operations overseas. Separately, the increasingly hawkish stance by the U.S. Federal Reserve amid rising inflation is also hurting high-multiple software stocks including Palantir.\nSo, is Palantir stock worth a look at current levels of about $19 per share? We think it is. Although Palantir’s revenue growth may cool versus historic levels, longer-term earnings growth potential looks good. Palantir sees itself growing revenues at 30%-plus levels from 2021 through 2025, as both its commercial and government businesses continue to expand. Margins are also looking up, with adjusted operating margins, which exclude the impact of stock-based compensation, standing at 32% for the first nine months of this year, up from around 11% last year, meaning that the company should be solidly profitable as revenues scale up. We value the stock at about $29 per share, or about 28x projected FY’22 revenues. See our analysis Palantir Valuation: Is PLTR Stock Expensive Or Cheap? for more details. Also, check out our analysis on Palantir Revenue for more information on the company’s business model and how it makes money.\n[11/26/2021] What’s Next For Palantir Stock After 17% Decline Last Month?\nPalantir stock (NYSE:PLTR) has declined about 17% over the last month, compared to the S&P 500 which was up by about 3% over the same period. The decline comes despite Palantir’s stronger than expected Q3 results, which saw the company grow sales by 36% year-over-year to $392 million while boosting its full-year revenue guidance to $1.527 billion, translating into year-over-year growth of 40%. However, investors are likely concerned that the company’s overall growth rate is cooling, considering that growth stood at a much healthier 49% over the first half of 2021. Moreover, the company’s commercial business outside of the U.S. also appears to be somewhat slow to gain traction. Although Palantir doesn’t provide a geographic revenue breakdown for commercial sales, it noted that U.S. commercial sales rose 103% year-over-year, while overall commercial sales grew 37% year-over-year.\nPROMOTED\nSo is Palantir stock likely to decline further in the coming weeks and months or is a rally looking more likely? Per the Trefis machine learning engine which analyzes historical stock price movements, PLTR stock only has a 36% chance of a rise over the next month (21 trading days). See our analysis Palantir Stock Chance of Rise for more details.\nFive Days: PLTR -6.1%, vs. S&P 500 0.3%; Underperformed market\n(23% event probability)\nMORE FROMFORBES ADVISOR\nBest Travel Insurance Companies\nByAmy DaniseEditor\nBest Covid-19 Travel Insurance Plans\nByAmy DaniseEditor\nPalantir stock declined 6.1% over a five-day trading period ending 11/24/2021, compared to the broader market (S&P500) which rose 0.3% over the same period.\nA change of -6.1% or more over five trading days has a 23% event probability, which has occurred 66 times out of 286 in the last year.\nTen Days: PLTR -6.3%, vs. S&P 500 1.3%; Underperformed market\n(28% event probability)\nForbes Money\nIncome Based Repayment:\nAdvocates Call On Biden To Enact Sweeping Reforms For Student Loan Borrowers\nPalantir stock declined 6.3% over the last ten trading days (two weeks), compared to the broader market (S&P500) which rose by 1.3%.\nA change of -6.3% or more over ten trading days has a 28% event probability, which has occurred 79 times out of 281 in the last year.\nTwenty-One Days: PLTR -17%, vs. S&P 500 3%; Underperformed market\n(8% event probability)\nPalantir stock declined 17% over the last twenty-one trading days (about one month), compared to the broader market (S&P500) which rose by 3%.\nA change of -17% or more over twenty-one trading days has a 8% event probability, which has occurred 22 times out of 270 in the last year.\nLooking for reasonably valued software stocks with big room to grow? Check out our theme on Mid-Cap SaaS Stocks\n[10/6/2021] Palantir Stock Gains 10% On U.S. Army Contract, But There’s Still More Upside\nThings have been looking up for big data and analytics player Palantir (NYSE:PLTR) in recent months, with the company posting robust Q2 earnings and improving the growth rates for its closely tracked commercial operations as business picks up with the Covid-19 reopening. Things are looking still better for the company’s bread-and-butter government business, as well. On Tuesday, Palantir said that it won the bulk of an $823 million indefinite-delivery, indefinite-quantity contract to provide data and analytics software to the U.S. Army, using its Gotham platform. This is a reasonably large win, considering that Palantir revenues are projected to stand at just about $1.5 billion in FY’21, per our estimates. Palantir stock rallied by about 10% in after-hours trading following the announcement, trading at $25.60 per share.\nSo is there more room for growth in Palantir’s stock? We think Palantir stock has a little more upside. We value the stock at about $29 per share, or about 35x projected 2021 revenue, a premium of about 13% versus the after-hours stock price as of Tuesday. We think the premium is justified, given Palantir’s solid growth outlook and its improving margins. Palantir sees itself growing revenues at 30%-plus levels from 2021 through 2025, as both its commercial and government businesses continue to expand. Margins are also looking up, with adjusted operating margins standing at over 30% for the last three quarters straight, meaning that the company should be solidly profitable as revenues scale up. See our analysis Palantir Valuation: Is PLTR Stock Expensive Or Cheap? for more details.\n[9/20/2021] Up 14% Last Month, What’s Next For Palantir Stock?\nBig data and analytics player Palantir’s stock (NYSE: PLTR) rallied by over 9% over the last week (five trading days) compared to the S&P 500 which declined by about 1% over the same period. The stock also remains up by about 14% over the last month. There are a couple of developments driving the recent gains. Firstly, investors are likely continuing to buy into the Palantir stock following its stronger-than-expected Q2 earnings which saw the company post robust commercial growth and raise its full-year guidance. Moreover, last week Palantir said that it was working with Wejo, a specialist in connected vehicle data, to combine Wejo’s massive data sets with Palantir’s Foundry data analytics platform, to provide insights that can be leveraged by auto parts suppliers, regulators, and city planners. Although the size of the market isn’t clear, it underscores that Palantir’s big data platform has considerable opportunity beyond the core government market.\nSo will the rally continue or is a decline looking more likely in the near term? Per the Trefis machine learning engine, out of 74 instances in the last year that Palantir Technologies (PLTR) stock saw a 21-day rise of 14% or more, 47 of them resulted in PLTR stock declining over the subsequent one-month period (21 trading days). This historical pattern reflects 47 out of 74, or about 64% chance of a drop in Palantir Technologies stock over the next month. See our analysis Palantir Stock Chance Of Decline for more details.\nThat said, we think Palantir stock is actually fairly valued at current levels of $29 per share. While the stock trades at about 36x projected 2021 revenues, this is largely justified by strong growth rates (annual top-line expected to grow at 30%-plus levels through 2025), expanding margins, and new opportunities in the commercial business. See our analysis Palantir Valuation: Is PLTR Stock Expensive Or Cheap? for more details on the company’s revenues, the valuation multiple, and comparison with peers.\n[9/3/2021] Is Palantir Stock A Buy At $26?\nBig data and analytics player Palantir’s stock (NYSE: PLTR) rallied by over 6% over the last week (five trading days) compared to the S&P 500 which gained a little over 1% over the same period. Although there hasn’t been too much news specific to Palantir stock over the past few days, the broader software space has performed well and investors are also likely continuing to buy into the Palantir stock following its stronger-than-expected Q2 earnings report, published on August 20. So will the rally continue? We think Palantir stock has a little more upside. We value the stock at about $29 per share, or about 35x projected 2021 revenue, a premium of about 10% versus the current market price.\nThere are a couple of trends driving our valuation. Firstly, Palantir’s long-term outlook is strong, with the company forecasting that its annual top-line will grow at 30%-plus levels through 2025. Moreover, the company’s commercial business, which had been a mixed performer over the last year, is seeing increasing traction with revenue rising by about 28% year-over-year during Q2, and this traction could hold up, as economies begin to open up following Covid-19 lockdowns with spending by businesses poised to look up. A possibility of an increasing commercial revenue mix is positive for Palantir as the commercial business is much more transparent compared to the government vertical, with valuation multiples for commercial businesses also usually higher than for government contractors. Thirdly, margins are also picking up nicely. Adjusted operating margins rose to about 33% over the first half of 2021 versus about 3% last year, meaning that Palantir could be solidly profitable as revenues continue to scale up. See our analysis Palantir Valuation: Is PLTR Stock Expensive Or Cheap? for more details.\n[7/13/2021] Palantir’s Commercial Business Is Picking Up. What Does It Mean For The Stock?\nPalantir (NYSE:PLTR) published a stronger than expected set of Q2 2021 results as it continued to see robust demand for its software and services. Total revenues rose by about 49% year-over-year to $376 million, while adjusted earnings stood at $0.04 per share, up from $0.01 per share last year. Palantir stock rallied by about 11% in Thursday’s trading.\nNotably, the company’s commercial business, which had been a mixed performer over the last year, saw revenue grow by about 28% year-over-year, as sales to customers in the U.S. soared. This marks an increase from a year-over-year growth rate of about 19% during Q1. Although Palantir’s bread-and-butter government revenue actually grew much faster at 66% year-over-year, driven by new deals with the U.S, investors typically pay a lot more attention to the commercial side of Palantir’s business. This is because the commercial business is more transparent compared to the government vertical, and valuation multiples for commercial businesses are also usually higher than for government contractors.\nThings could get better still for the commercial business going forward, as Palantir has been ramping up its direct sales force, expanding its distribution channels, and shifting from a highly customized deployment model to a slightly more standardized model. Moreover, the company’s international operations, which remained muted over Q2, could see a recovery going forward, as economies begin to open up with spending by businesses poised to look up.\nWe value Palantir stock at about $27 per share, slightly ahead of the current market price. See our analysis Palantir Valuation: Is PLTR Stock Expensive Or Cheap? for more details on the company’s valuation and comparison with peers.\n[6/30/2021] What Are The Catalysts For Palantir’s Stock’s Next Big Breakout?\nGovernment-focused big data and analytics player Palantir (NYSE:PLTR) saw its stock rally following its September 2020 IPO, rising from levels of under $10 per share to over $38 by mid-February 2021. However, the stock has declined since then and has largely been trading sideways at levels of between $20 and $30 per share over the last few months. The correction was driven by a slightly weaker than expected growth outlook for 2021 and also due to the broader sector rotation happening in the markets, with investors moving funds away from high-growth stocks into value and cyclical stocks, to play the re-opening following Covid-19, and also due to an increasingly hawkish stance by the U.S. Federal Reserve. However, we see a couple of catalysts over the near-to-medium term that could drive Palantir stock higher in the coming months.\nPalantir has posted net losses every year since it was founded back in 2003. Reported net losses stood at about $1.17 billion in 2020, although this was largely due to high stock-based compensation expenses. It’s likely that the tag of being a relatively mature, yet unprofitable company isn’t helping the narrative around Palantir stock. However, the economics of Palantir’s underlying business is clearly getting better. Adjusted operating margins rose to about 17% in 2020, up from negative levels in 2019 and the company also recently raised adjusted free cash flow guidance for the full year 2021, from breakeven to in excess of $150 million. As revenues continue to scale up it’s likely that profits will also follow suit in the coming quarters, potentially proving a catalyst for the stock.\nAnother big catalyst is likely to be a stronger performance of Palantir’s commercial operations. The commercial business, which was Palantir’s biggest segment over 2018 and 2019, has fallen behind the government vertical, which has now emerged as the company’s largest, and fastest-growing business. Over Q1 2021, commercial revenues stood at $133 million, up just about 19% year over year, well below the government business, which grew by 76% to $208 million. This is problematic for Palantir, as the government business has lower levels of transparency, with investors also valuing government contractors at lower multiples. Now Palantir is taking multiple steps to drive commercial growth, by ramping up its direct sales force, expanding its distribution channels, and shifting from a highly customized deployment model to a slightly more standardized model. If these efforts pay off, it could help Palantir’s stock, as well.\nLooking for reasonably valued software stocks with big room to grow? Check out our theme on Mid-Cap SaaS Stocks\n[5/30/2021] What Ails Palantir’s Government Business?\nA big part of the investment thesis for government-focused big data and analytics player Palantir (NYSE:PLTR) hinges on the expansion of its commercial operations. However, recent progress has been somewhat mixed. The commercial business, which was Palantir’s largest segment over 2018 and 2019, has fallen behind the government vertical, which has now emerged as the company’s largest, and fastest-growing business. Over Q1 2021, commercial revenues stood at $133 million, up just about 19% year over year, well below the government business, which grew by 76% to $208 million. This is problematic for Palantir, as the government business has lower levels of transparency and comes with perception issues. Investors also tend to value government contractors with lower multiples. For example, the stock declined by almost 10% in the days following Palantir’s Q1 results, as investors brushed off the company’s earnings beat and strong guidance and focused on slower than expected commercial growth.\nThat said, we think things could look up from here. Palantir’s commercial business is primarily comprised of its Foundry software product – which it markets as a central operating system for an organizations’ data, that enables users to integrate and analyze the data they need in one place. The software finds applications in industries ranging from health care to manufacturing. Although companies have scaled back on IT spending over 2020, as they focused budgets on remote working and collaboration tools, they could renew their focus on other areas such as big data and analytics from this year onward.\nPalantir is also taking multiple steps to drive commercial segment growth. The company has been increasing its direct sales force, while also expanding its distribution channels with new partnerships with complementary software companies, system integrators, and cloud providers in a move that could increase its reach considerably. The company announced a distribution deal with IBM over the last quarter. Palantir has also taken steps to shift from a highly customized, time-consuming deployment model that relies on engineers to configure, maintain, and upgrade its tools, to a slightly more standardized model. There’s plenty of room to scale up the commercial business as Palantir estimates the total addressable market for its commercial operations at about $56 billion, just slightly below the $63 billion addressable market for the government vertical.\nSee our interactive analysis on Palantir’s Valuation for more details on the company’s revenues, valuation, and how it compares with other analytics and software players. You can modify key variables to arrive at your own price estimate for Palantir.\n[5/13/2021] \nPalantir (NYSE:PLTR), a company best known for its big data and analytics tools for government and large corporations, saw its stock decline by around 11% over the last week. Although the company actually posted stronger than expected Q1 2021 results on Tuesday, the stock was likely impacted by two factors. Firstly, investors don’t seem too pleased with the company’s continued over-reliance on the government sector for growth. Second, the broader sell-off in high multiple technology stocks over the last few days also appears to have hurt Palantir. So is Palantir stock looking like a buy at current levels of $18 per share? While we think Palantir stock remains a little expensive, trading at about 24x projected 2021 revenues, and ahead of our $16 price estimate, the risk to reward trade-off is looking much better after the recent correction.\nSee our interactive analysis on Palantir’s Valuation for more details on the company’s revenues, valuation, and how it compares with other analytics and software players. You can modify key variables to arrive at your own price estimate for Palantir.\nAlthough Palantir’s government business continues to overshadow the commercial operations, with the business growing by 76% year-over-year to $208 million in Q1 2021, the company has made some progress in the commercial space as well. Over Q1, the commercial sales grew 19% year-over-year to $133 million, well ahead of the 4% growth rate it clocked in the previous quarter. Revenue per commercial customer is also picking up, rising by about 29% compared to last year. More significantly, revenues from the top 20 customers grew even faster at 34%, indicating that loyalty is strong, with customers seeing more value in the company’s offerings. The outlook for the commercial business should also look up, considering that Palantir is significantly hiring for sales positions and recently formed a sales partnership with enterprise computing behemoth IBM. Moreover, the overall economics of Palantir’s business is looking better, with adjusted gross margins rising to 83% from 75% last year, with the company raising its adjusted free cash flow guidance for the full year, from breakeven to in excess of $150 million.\n[3/26/2021] What’s Happening With Palantir Stock?\nPalantir stock (NYSE:PLTR) has declined by about 40% from its February highs, currently trading at around $22 per share. The decline is driven in part by the broader sell-off in high growth stocks, mixed December quarter earnings, and the expiration of the company post-IPO lock-up last month, which has enabled insiders to sell shares. Separately, recent comments by the company’s CEO asking investors with a short-term focus to consider other stocks are also likely to have hurt the stock. So does Palantir stock look like a buy after this correction? We don’t think so. While the revenue outlook remains quite strong, with the company on track to grow at 30% plus levels each year over the next two years, the company’s valuation multiple is too high, in our view, with the stock trading at about 28x forward revenues. Palantir is also not yet profitable, despite the fact that it has been around for around 17 years now. Separately, the company is getting more dependent on government customers, rather than scaling up its corporate user base. For perspective, over Q4 2020, government revenue grew by 85% year-over-year to around $190 million, while the commercial revenue expanded just about 4% year-over-year to about $132 million.\nSee our interactive analysis on Palantir’s Valuation for more details on the company’s revenues, valuation, and how it compares with other analytics and software players. You can modify key variables to arrive at your own price estimate for Palantir.\n[2/17/2021] Palantir Stock Updates\nPalantir (NYSE: PLTR) stock declined by about 13% in Tuesday’s trading and is down by almost 25% over the last 5 trading days. The sell-off is largely driven by a mixed December quarter earnings report. Although Palantir’s quarterly revenues came in stronger than expected, it posted a quarterly loss. Palantir’s revenue guidance for the full year 2021 was also lower than expected, with the company expecting sales to grow at over 30%, slowing significantly from the 47% growth rate it saw in 2020. [1] Although 30% growth isn’t bad, Palantir’s stock is priced for higher growth, trading at almost 35x forward revenues. Secondly, the company doesn’t seem to be reducing its dependence on government space. Over Q4, government revenue grew by 85% year-over-year to around $190 million, while the commercial segment underperformed with revenue growing just about 4% year-over-year to about $132 million. Separately, Palantir also has a post-IPO lock-up period expiring later this week, and this is expected to increase the number of shares available for trading. It’s possible that this is also putting pressure on the stock.\nSee our interactive analysis on Palantir’s Valuation for more details on the company’s revenues, valuation, and how it compares with other analytics and software players. You can modify key variables to arrive at your own price estimate for Palantir.\n[1/20/2021] Up 2.5x Since Its IPO, What Really Changed For Palantir Stock\nPalantir (NYSE: PLTR) stock had a solid run since last November, rising from levels of $10 to around $25 currently, after trading relatively flat in the weeks following its public offering in late September. So what has really changed for Palantir stock in recent months to justify this change?\nAlthough sectors such as cloud-based software and big data and analytics have been much sought after with investors through Covid-19, Palantir was one of the few software names that saw little movement post its IPO. The stock looked like a decent value back in October trading at roughly 15x projected 2020 revenues. The reasonable valuation, slightly stronger than expected Q3 2020 earnings published in November, and a series of contract wins (albeit small) likely helped the narrative around the stock, helping to build momentum and gain visibility with investors. Moreover, investors seem to be appreciating Palatir’s efforts to shift from a highly customized, time-consuming model that relies on engineers to configure, maintain, and upgrade its tools, to a slightly more standardized model. For instance, on its Form S-1, the company noted time required for its software to get up and running declined more than five-fold since Q2 2019 to an average of 14 days in Q2 2020. The standardization and lower resource usage are beginning to reflect on the company’s margins. Over Q3 2020, Adjusted Gross Margins expanded to 81% up from 70% last year. Such standardization should also help the company scale up its customer base particularly in the commercial space, where the company estimates its addressable market at about $56 billion. [2]\nThat said, Palantir does look expensive at current valuations trading at over 30x projected 2021 Revenue presently. See our interactive analysis on Palantir’s Valuation for more details on the company’s Revenues, current valuation, and how it compares with other analytics and software players.\n[1/7/2021] Why Did Palantir Stock Decline 20% Over The Last Month?\nThe stock price of big data and analytics player Palantir (NYSE: PLTR) is down by about 20% over the past month. Although the company has seen some positive developments in recent weeks, including a new two-year contract with the UK National Health Service and a renewal of a previous deal with the U.S. Army that is valued at about $114 million for a year, the stock is being weighed down by a couple of factors. Firstly, some sell-side analysts have turned bearish on the stock, citing its high valuation. Palantir is up around 2.5x since its IPO and trades at about 28x projected 2021 Revenue. Secondly, Palantir’s IPO lockup period – through which insiders are forbidden to sell their holdings – will expire following its next earnings release, likely in mid-February. While Palantir went with a direct IPO that enabled insiders to sell 20% of their holdings at the time of listing, they will be free to sell the other 80% of their holdings as the lockup expires, boosting the supply of the stock. It’s possible that this could also be weighing on the stock.\nSee our interactive analysis on Palantir’s Valuation for more details on the company’s Revenues, current valuation, and how it compares with other analytics and software players.\n[12/4/2020] Palantir Stock’s 2x Rise Isn’t Warranted\nPalantir (NYSE: PLTR) stock has seen some big moves in recent weeks. While the stock had a muted debut post its September IPO, partly due to its direct listing that enabled insiders to sell their holdings, bolstering initial supply, the stock has more than doubled over the last one month, rising from levels of around $11 in early November to about $24 currently. So what has been driving the stock in recent weeks? While there hasn’t been a big change to the company’s fundamental picture to warrant such a jump, a combination of high retail investor interest in big data and analytics companies and marginally better than expected Q3 2020 earnings figures likely helped the stock. Over Q3, Revenues rose 52% year-over-year as the company continued to gain ground in the Government vertical (revenue up 68% year-over-year) with Commercial sales growing 35%.\nWhile we thought Palantir’s valuation was attractive at levels of about $10 (see our comparison of Palantir and Snowflake below), we think its richly valued right now. Palantir now trades at about 40x projected 2020 Revenues. While this is still behind the likes of Datadog (which trades at 50x estimated 2020 Revenues) and Snowflake (over 150x), Palantir does have some unique risks. Firstly, Palantir typically benefits from significant economic and geopolitical uncertainty, and the Covid-19 pandemic and the related recession were likely big drivers of growth this year. However, with the availability of a highly effective Covid vaccine looking likely by early 2021, things could start to return to normal, potentially hurting growth. Secondly, Palantir remains highly dependent on government contracts (about 55% of total Revenue) – particularly in areas related to surveillance and national security – causing transparency and perception issues. Palantir’s most recent report indicates that the company is actually increasing its exposure to this space. Also, as we’ve noted previously, Palantir’s products don’t scale as seamlessly as other SaaS players, as they need to be adapted to the unique needs of customers. This could also hurt long-term growth.\nSee our interactive analysis on Snowflake’s Valuation and Palantir’s Valuation for more details on the two companies’ valuation.\n[Updated 11/9/2020] Why Did Palantir Stock Soar?\nPalantir (NYSE: PLTR) stock rallied by about 35% over the last week, trading at levels of about $14 per share, after remaining largely listless post its late September debut. Big data and analytics is a hot sector at the moment, though investors have been on the fence about Palantir’s stock, given its high exposure to government contracts and also due to questions regarding the company’s ability to scale-up its user base. While it’s difficult to pinpoint what exactly caused the jump last week, there could be a couple of factors.\nThrough Covid-19, Palantir has been seeing higher traction from the public health space, with its services used to track Covid-19 data from hospitals and to trace the spread of the virus. The company is also developing tools to help authorities with the logistics related to Covid vaccines. Last week it was reported that the company was in talks with the U.K government to support its contact tracing efforts in the country. This could give investors some confidence that the company is diversifying its revenue streams to an extent within the government space to areas that have lower transparency and perception issues. With the company’s Q3 earnings due on November 12, investors are likely anticipating a strong quarter.\n[Updated 10/21/2020] Snowflake Vs. Palantir\nThe last month saw Palantir (NYSE: PLTR) and Snowflake (NYSE: SNOW) – two relatively high-profile software players go public. Snowflake’s software enables organizations to manage and analyze large quantities and diverse types of data across public clouds such as Amazon’s AWS in a single, easy-to-use platform. Palantir offers big data and analytics solutions primarily used by governments and intelligence agencies, although it has been expanding its presence in the commercial space.\nWhile the two companies are focused on big data, investors are valuing them very differently. Snowflake stock trades at over 120x projected FY’21 Revenues (FY ends January) while Palantir trades at just about 15x projected FY’20 Revenues (FY end December). Does this make sense? How do the companies compare in terms of business models, revenue growth rates, and margins? We provide more details below.\nSee our interactive analysis on Snowflake’s Valuation and Palantir’s Valuation for more details on the two companies’ valuation.\nRevenues & Growth Rates\nPalantir’s Revenues grew by 24% to about $740 million in 2019 and growth is likely to pick up to levels of over 40% in 2020 as Covid-19 related disruptions increased demand for the company’s services. In comparison, Snowflake saw Revenue grow 173% from $97 million in FY’19 to about $265 million in FY’20, although the growth rate is likely to slow down to roughly 110% over the current fiscal, based on consensus figures. Overall, Snowflake’s Revenues should grow at a higher rate compared to Palantir, considering its SaaS-based model which can scale to a large base of customers with much less customization. Palantir, on the other hand, needs engineers to adapt its tools to the unique needs of customers. Snowflake had over 3,100 customers as of July 2020, compared to Palantir which had about 125 customers as of its last fiscal year.\nProfitability \nWhile Palantir is slightly ahead in terms of profit margins considering that it is the more mature company (Palantir was founded in 2003 versus Snowflake which was founded in 2012), we expect Snowflake to be more profitable in the long-run given its relatively more standardized product and lower customer acquisition costs. Snowflake posted a Gross Profit Margin of 62% for the first six months of FY’21, with Operating Margins standing at -72%. Palantir’s Gross Margins stood at about 72% over the first six months of 2020, with Operating Margins coming in at about -35%.\nValuation \nSnowflake stock has more than doubled from its IPO price of $120 to about $250 currently, valuing the company at about $70 billion. Palantir, on the other hand, hasn’t moved too much since its listing and is valued at about $15 billion. There are a couple of reasons for Snowflake’s premium valuation. Firstly, the company is growing much faster than Palantir and should also be more profitable in the long-run given its highly scalable delivery model. Investors have also been paying a big premium for growth stocks. Secondly, unlike Palantir which has high exposure to government contracts – particularly in areas related to surveillance and national security – causing transparency and perception issues, Snowflake’s business is focused on more commercial customers.\nThat said, Snowflake has considerable valuation risk, considering that it trades at about 122x projected FY’21 revenues, compared to Palantir which trades at just about 15x projected 2020 Revenues. The story could change quickly. If Snowflake’s growth rates slow down, with the company facing competition from cloud majors such as Amazon and Google who offer their own data warehousing solutions, investors could re-think its valuation. On the other side, investors could double down on Palantir stock if they see more proof points indicating that the company is making progress in the commercial sector, via high-profile deals or stronger Revenue growth.\nWhat if you’re looking for a more balanced portfolio instead? Here’s a high-quality portfolio that’s beaten the market consistently since the end of 2016.\nReturn TREFIS\nInvest with Trefis Market Beating PortfoliosSee all Trefis Price EstimatesNotes:\nPalantir Press Release [↩]\nPalantir Form S-1 [↩]', 'By Oliver Schoenborn, SAP\nThe phrase “data is the new currency of the digital age” has been coined for years as though data is an asset that every business should chase and capture. But for an asset so precious, data loses its value incredibly fast when it is not “spent” as soon as it is acquired.\nThe disparity between data hoarding, and data sharing, blocks potential intelligence that growing businesses need to support faster, more accurate decision-making and strategic action. And as the volume, variety, and velocity of information continue to grow exponentially, the traditional rules-based legacy ERP system increasingly becomes a risky liability as more data is left unused.\nAccording to IDC, Intelligent ERP is the key to overcoming this technical deficit. This technology enables midsize businesses to balance time-tested rules with automation, self-learning, and real-time situational prediction by supporting use cases that combine Big Data with analytics and machine learning.\nConnecting, embedding, and revealing data’s full value\nOne of my favorite examples that thoroughly showcases the benefit of connecting and embedding data in real time is a use case adopted by Zalando Payments GmbH (ZPS). With a staff of 235, the payment services provider processes millions of consumer transactions across over 17 countries for fashion retailer Zalando SE. But what makes ZPS’s story particularly unique is its business model: a factoring service that provides consumers the flexibility of more than 20 payment options.\nZPS’s business model gives it a competitive advantage over companies that have more simplistic approaches to payment processes. However, it also requires the provider to comply with German financial regulations to protect merchants and consumers from bankrupt providers. One of those standards mandates an on-demand, accurate computation and monitoring of the total funds held in its bank accounts belonging to third parties – a task that its existing technology couldn’t accommodate.\nWhile challenging, this requirement led to an innovation that helped the payment services provider optimize its financial operations and better understand and expand its business. ZPS collaborated with the University of Seville in Spain to build a customized cash-flow model to uncover valuable liquidity and financial planning insights.\nWithin this guarantee-monitoring model, ZPS uses Intelligent ERP to replicate data on contract accounts receivable in near-real time to a business warehousing solution and other reporting applications. An in-memory database then processes the data, calculates key figures such as customer cash-in and factoring cash-outs, and uses these figures to determine the amounts to be guaranteed each day. Furthermore, with a live connection to its business warehousing solution, ZPS uses a cloud-based analytics solution to let employees access calculated data and consume reports through intuitive dashboards and predictive stories.\nBy amplifying the value of its Big Data with Intelligent ERP and augmented analytics, ZPS allows a larger circle of business users to gain insights into financial KPIs, such as gross customer cash-ins or days from order. But this is not the end of the company’s transformation story: ZPS is now looking to develop a predictive model that forecasts amounts to be guaranteed in the future.\nDriving true business transformation begins with Big Data\nWhile picking the right technology strategy is a complex process made even more complicated by the evolving needs of a growing business, companies can never go wrong making choices that optimize their data. And one of those technologies – as demonstrated by ZPS’s recent transformation success – is Intelligent ERP. \nFor midsize companies, adopting Intelligent ERP can kick off a digital transformation that goes beyond addressing a specific business need. It also provides the process efficiency, operational visibility, and actionable insights that the broader workstream needs to help grow the business.\nIf you are interested to know more about how Intelligent ERP can help growing companies rethink their approach to digital transformation, review a recent IDC article “Intelligent ERP: Delivering Critical Business Capabilities That Current Systems Lack”, and take a look at my recent blogs:\n5 Warning Signs Of A Growing Business That’s Outgrown Its Legacy ERP\nWhy Intelligent ERP Systems Are The Secret To Growing A Company’s Competitive Edge', "Data networks coordinating commercial rooftop solar arrays GETTY\nWhen you think about solving the climate crisis, what springs to mind?\nMost people’s knee-jerk reaction is along the lines of “electrification,” “carbon sequestration,” “recycling,” or “renewable agriculture.”\nWhile not many think of phrases like “big data” or “artificial intelligence,” several recent conversations have convinced me how important these fields are to helping our civilization thrive and survive into the next century.\nThe two founder / CEOs with whom I have had the pleasure to speak recently use AI in very different ways and in completely different fields, but it is clear that the ubiquity of cheap computing power, combined with smart engineers and focused, visionary entrepreneurs represents a formidable force in helping us mitigate and adapt to today’s harsher, more challenging post-climate world.\nPROMOTED\nThe companies featured in this article are Clir and SINAI Technologies.\nClir\nPALM SPRINGS, CALIFORNIA - FEBRUARY 27, 2019: Wind turbines generate electricity at the San Gorgonio ... [+] GETTY IMAGES\nMORE FOR YOU\nSinai Technologies Is Using Big Data To Address The Agricultural Emissions Gap\nA Smart Way To Provide Long-Term, Grid-Scale Storage: Hydrostor\nThese Are The Startups Applying AI To Tackle Climate Change\nEveryone knows that one big downside of renewable energy (RE) generation is intermittency.\nWhile the grid can cushion some of the ill-effects of intermittency through large-scale battery installations, varying production levels inevitably add uncertainty to the operation of RE facilities.\nUncertainty has negative consequences for plant operators and grid managers (who must instantaneously match power supply to power demand), but it also has negative consequences for financiers of renewable energy projects.\nForbes Innovation\nNutanix Rajiv Ramaswami On His\nFirst Year As CEO\nWhen owners take on leverage (i.e. borrow money) they provide lenders with forecasts of energy production and prices. Inaccurate forecasts can result in banks charging higher interest rates and insurers higher premiums. M&A deal flow also depends on quickly and accurately assessing the production potential of RE assets.\nOne start-up, Clir, has been using innovations in data science and artificial intelligence to understand operational drivers at renewable energy plants, then leveraging that understanding to improve plant efficiency and decrease uncertainty. By doing so, Clir can bring down the cost of capital for clean electricity generators — making RE facilities more attractive investment assets.\nClir consolidates data from all the units in a wind farm or solar array – a truly enormous amount of data – and runs that data through machine learning algorithms to get a picture of how the facility operates over time and in different environmental conditions.\nClir’s AI then identifies ways to maximize the overall efficiency of the facility in ways that CEO Gareth Brown says might sometimes seem counter intuitive.\nFor example, for offshore wind farms or those in the middle of large US deserts, there is not much mixing between air at different elevations. In these low-mix cases, efficiency for the farm overall increases when the leading turbines are set to operate at less than peak efficiency.\nThe wind left un-churned by the leading, wake-creating turbines hits the blades of the trailing, wake-affected turbines with greater force, generating more power. The power generated by the wake-affected turbines more than offsets the reduction in power from the wake-creating ones. According to academic research, this process, known as “wake steering,” can increase the power generated by around 10% and, more importantly, decreases the variability of the power generated by around 70%.\nA 70% decrease of uncertainty represents a big win for asset owners and the financiers that back them. With operational uncertainty decreased, banks and insurers can better assess the potential risks and returns, and price their financial products more appropriately. Investors looking to acquire renewable energy assets also have a better idea what a fair price to pay is.\nThe main premise behind this column is that — insofar as it represents the economic manifestation of humanity’s ability to adapt — capitalism is an irreplaceable tool for fighting climate change. Capitalism routes money toward the most successful ideas, so to the extent that Clir’s AI is helping investors make sound capital allocation decisions, it is at the tip of the spear in civilization’s transition to a renewable energy future.\nSINAI Technologies\nSINAI's AI-powered decarbonization platform SINAI TECHNOLOGIES\nFor years, company managers had to concern themselves with a single, straightforward task: maximize profits for the company’s owners. While not simple, this task was made a lot less tricky because governments did not know they should be charging companies for spewing greenhouse gases (GHG) into the atmosphere.\nGovernments finally began to announce and implement carbon taxes over the last decade, but there is still no universal solution. Different country-level or regional carbon tax schemes force some companies in some industries to pay for some GHG emissions; coverage is spotty, and enforcement differs from one jurisdiction to another.\nWith recent announcements about border adjustment taxes in both the US and Europe, it looks like the two largest trading blocks are finally on the verge of forcing companies to “internalize” the costs associated with GHG waste.\nWhile carbon border taxes would be undiluted good news for enthusiasts of life as we know it, the imposition of these taxes is forcing companies to completely rethink their capital spending and operational planning. Large companies are moving toward instituting internal carbon pricing (e.g., Microsoft, Danone), by which divisional and firm-wide profits are adjusted by an assumed cost of GHG emissions.\nCorporate planning centered on internal carbon pricing is not a trivial task. Corporate accounting and planning systems were not set up to measure or report these costs, so just gathering the data is challenging and highly manual. Companies spend big bucks on armies of consultants to pull together ad hoc spreadsheets to try to gather the required data so that simple go / no-go kinds of decisions can be made.\nOne start-up – San Francisco-based SINAI Technologies, founded by Maria Fujihara (CEO) and Alain Rodriguez (CTO) – is aiming to change these manual processes and bring the field of de-carbonization into the 21st century.\nSINAI designs automation routines to compile emissions data from different corporate divisions into a common data store, so that the firm’s overall emission profile can be accurately assessed.\nIn addition to internal company data, SINAI gathers regional electrical generation data and information from suppliers to make estimates of a company’s baseline Scope I, II, and III GHG emissions.*  This allows SINAI’s clients a holistic view of their overall GHG footprint and insight into what part of the supply chain needs the most mitigation work.\nAfter data is collected and the baseline created, SINAI’s systems use artificial intelligence to forecast emission levels under different mitigation scenarios. Decarbonization-related capital spending plans can be made by comparing the mitigation effects of different technology implementations along different parts of the supply chain.\nThe planning process made possible by SINAI’s technology allows companies to make intelligent strategic decisions regarding what decarbonization projects they should focus on to get the biggest bang for the buck. Leveraging tools like this is vital if companies are to make the enormous changes necessary to adapt to our post-warming world.\nClir’s Brown and SINAI’s Fujihara and Rodriguez know, as I know, that in order to meet the challenges of the 21st century, we need to pull out all the stops and use all the tools in our toolkit — including big data analysis and AI.\nIntelligent investors take note.\nNOTE:\n* A company that manufactures a product or provides a service generates GHGs directly due to their business operations. These “direct” emissions are termed “Scope I” emissions. Every company that uses electricity to produce its good and services also shares some responsibility for emissions due to electricity generation. These are termed “Scope II” emissions. Last, for every company that needs inputs to produce a good or provide a service, GHG emissions are likely generated by the company producing those inputs. Those are called “Scope III” emissions.", 'As borders reopen, big data shows us where everyone is planning to go — and what to avoid. Why not skip the hotspots and instead experience the American outdoors, in all its autumnal splendor. \nSnowcapped mountains, USA GETTY\nIt’s open season in America. This week, the United States reopened borders to non-essential travel for the first time in 20 months. And travelers are raring to go. \nAlready, Delta Airlines reported a 450% increase in international bookings versus the six weeks prior to the announcement. Looking ahead, roughly 66% of Europeans plan to travel during the fall and winter seasons, according to a new poll by the European Travel Commission (ETC). As for where they’re all headed — the most popular travel destinations in the US are Los Angeles, Miami, New York, Orlando, and the South Florida Atlantic Coast, according to new data released by Airbnb this month. Google Travel data reflects similar results in popularity, with the addition of Joshua Tree National Park. \nIf you’re craving sun, rest, and wide open spaces coming out of this pandemic, you’ll want to avoid the chaos of the crowds. The following destinations are ancillary — each a few extra hours from the gateway cities that make America so accessible. But they’re well worth the effort, not for status or luxuries, but for their storied histories, natural wonders and soulful hospitality.\nPROMOTED\n1. Catskills, New York\nAutumn in the Catskills, New York. GETTY\nThis upstate New York idyll is named for its Catskill mountain range, a lush, unpeopled stretch of fall foliage now festooned in every shade of brown, red russet and gold. At sunrise and sunset, the entire landscape is bathed in an almost otherworldly light. The kind that makes you want to get up early just to bear witness, with a hot cup of coffee in your hands and the only sound the creak of your wooden rocking chair.\nMORE FOR YOU\nThe 10 Top Countries, Cities And Regions To Visit In 2022 According To Lonely Planet’s Best In Travel\nHere Are 3 Remarkable Things About The 2020 Alfa Romeo Stelvio That You Should Know\n34 Hotels To Visit This Holiday Season\nThat’s the spirit of the place: Arrive as strangers, leave as friends. At least, that’s the tagline for my personal favorite, the new Urban Cowboy lodge, which stakes its claim to about 70 acres alongside protected land known as Big Indian Forest Preserve, the largest stretch of uninterrupted forest in the Catskill Mountains. Owned and designed by Lyon Porter, the 28-room lodge is a Brooklyn hipster’s take on adult camp for urbanites looking to explore ‘wilderness.’ The joke is, you’re so close to organic restaurants, handcrafted bourbon cocktails, and oversized copper soaking tubs, no one here is actually roughing it. For a real forest bathing experience, head to one of the 12 hiking trails in the surrounding area, which vary in difficulty from beginner to advanced.\nIf you want a thrill on property, brave the outdoor sauna for a morning detox, emerge somehow renewed, run barefoot down the hill to the entrance and hop into a freezing cold fishing hole, which the locals call a creek. Because cold plunging — haven’t you heard — is the latest hipster health trend (that’s been around forever).\nForbes Innovation\nNutanix Rajiv\nRamaswami On His First Year As CEO\n2. Nantucket\nGreat Point Lighthouse on Nantucket GETTY\nThis 105-square-mile island off the coast of Cape Cod looks like a postcard from almost any angle. The cedar shingle houses, the cobblestone streets, the flower boxes in each window sill, the beach bike culture, and fresh caught seafood are just a few of the everyday charms that make this place so delicious and delightful. While most vacationers come for the summer season, some of the best attractions in town come to life in autumn and winter (such as lower room rates). \nIt takes a cold, clear night to see a stunning view of the Milky Way at Nantucket’s Loines Observatory. Its telescopes offer a window onto Saturn’s rings and Jupiter’s great red spot. But most exciting during my visit was a real live shooting star that streaked across the night sky before burning into stardust, like magic. And soon, the island will be laureled in holiday lights to host its annual Christmas Stroll, when all the local shops open their doors, Santa makes an appearance, and yuletide carolers fill Main Street with song. (It’s both tradition and a merry way to keep shoppers from heading off island). \nGetting here, too, is easier than you’d think. Just pack up the car, drive to Hyannis, Cape Cod and take the public ferry over to Nantucket in an hour. For a fancier trip, hop on a private Pilatus PC 12 plane with Tradewind Aviation, which typically run up to seven flights a day between Westchester airport (HPN) and Nantucket. \n3. Big Sky, Montana\nHorseback riding in Montana CREDIT JENNIFER LEIGH PARKER\nIf you’ve seen the tv series Yellowstone or the film A River Runs Through It based on Norman Maclean’s novel, you’ll have some sense of the rugged expanse that is Montana. There’s a reason the titans of the business world and Hollywood celebrities come here to hide out. It’s so wide open, the concept of privacy still exists. And when they say wilderness, they mean it — black bears, rainbow trout, eagles, and all. Apart from spectacular fly fishing, glamping, horseback riding and cattle ranching, you’ll also find world-class skiing to rival anything else in America.\nIt’s a little known fact that Big Sky Ski Resort has more skiable acres than Telluride and Jackson Hole combined. And it’s about to get better. On Dec. 15, Montage Big Sky, the area’s first five-star resort will open 139 guest rooms, suites and residences with ski-in, ski-out access. One&Only Resorts has also broken ground in Moonlight Basin, and when it opens (dates undisclosed) this will be the luxury brand’s first U.S. location. In the meantime, the best place to stay is called the Green O, in nearby Greenough, Montana. It’s the romantic and secluded adults-only outpost from the owners of Paws Up, a well known family resort that comes with all the trimmings (ATV rides, cattle drives, and multi course tasting menus). \nNow that Bozeman Yellowstone International airport welcomes direct flights from 29 major hubs, including New York City, Chicago, and Los Angeles — Montana is about to become America’s winter wonderland.\n4. Savannah, Georgia\nSavannah, Georgia, USA at Forsyth Park Fountain. GETTY\nYou’d be remiss to visit the States without a visit to the South, and its flowering sister city Savannah. To stroll beneath the live oaks and Spanish moss squares of this charming old town is a history lesson in itself. As this is the oldest city in Georgia, landmarks are the norm rather than the exception. If you want to know what happened here, just look around. So much of the American Civil War, slavery, and the evolution of the South is memorialized in its 19th century monuments, its graveyards, and its carriage houses. History is even carved into the red bricks that pave the wide easy sidewalks for curious passersby. \nThere are many things to do in Savannah — eating well and drinking heavily, chief among them. But young people are finding new ways to enjoy the entrepreneurial and artistic spirit of the place, and many decide to attend the private university Savannah College of Art and Design (SCAD) in midtown. As a tourist, my personal favorite stop is a visit to Jim Williams’ Mercer House Museum, which houses the collector’s still-intact private art collection. Williams is the subject of one of the most best-selling true crime novels of all time, Midnight in the Garden of Good and Evil by John Berendt. How much of the story is true, you can decide for yourself. But this is the place to ponder it.\n5. Calistoga, California\nPoolside at Solage resort in Calistoga, CA CREDIT JENNIFER LEIGH PARKER\nThis town has everything you’d expect from California wine country: panoramic mountain and vineyard views with a serious oenophile following. Specifically, this area is known for its bottles of big, bold Cabernet Sauvignon, as well as hand-crafted blends of Chardonnay, Pinot Noir, and Zinfandel. (Leave the chewing gum at home). \nThough it is located in Napa county, Calistoga has a small town feel compared to the commercialism of downtown Napa. It is peppered with hot springs and hiking trails, and you only need a bicycle to see the entire town. Lately, the big news here is a new 85-room resort. This November marks the grand opening of the Four Seasons Napa Valley Resort and Residences (so named despite its Calistoga location).\nIt is the only resort situated within a working winery on a 4.7 acre vineyard, meaning you can now enjoy tastings and tours without getting in a car. The best perk? As a Four Seasons guest, you’ll get priority reservations at the Elusa Winery and its restaurant during harvest season, a time when most Napa tours are sold out months in advance.\nFurther Reading: First Look: Delta, TSA Launch Facial Recognition At Atlanta Airport', "CEO of Think Tank Innovations, a communications solutions company, known for ShareSmart for virtual health appointments and consults.\nGETTY\nAdvancements in the technological capabilities of data generation, from sequencing DNA to health watches, have led to the phenomenon of big data. Big data refers to data that is rapidly generated, remarkably large and difficult to accurately interpret. Access to these kinds of data stores is revolutionizing many industries, such as banking, agriculture and science. As a result of their applications in healthcare, the field of life sciences is becoming one of the biggest users of supercomputers, which are being used to effectively store, manage and interpret data. \nThe Covid-19 pandemic has especially highlighted the potential of utilizing technology to increase efficiency with remote patient care and telehealth. Considering the popularity of virtual health; it is clear that the healthcare industry will increasingly rely on artificial intelligence and big data to improve gaps in our healthcare system. The switch to electronic health records in clinics has opened up the possibility of applying data models to actively use this information to provide proactive healthcare instead of this information remaining as a large store. Thus, the concept of a “data-driven physician” is gaining traction, since physicians can access more clinical data than ever before. Clinicians could then have earlier access to critical health information that enables them to manage conditions before a crisis occurs and mitigate a poor prognosis. \nPROMOTED\nBig data in healthcare could serve a multitude of purposes. Similar to the banking industry, AI and data models could be used to recognize both external (third parties) and internal (unauthorized healthcare workers) patient data breaches. In terms of patient care, analytic models are being developed and tested for risk prediction and diagnostic accuracy, ultimately working to minimize physician errors. These processes could be continued to be developed to scan a patient's file for lab values and other determinants then notify the doctor which patients are most at risk for certain diseases. Furthermore, the use of this data could contribute to placing more emphasis on dry lab practices compared to wet lab practices, which could be more economical. Overall, AI could be used to turn mass stores of patient data into proactive solutions that assist physicians with offering a higher level of comprehensive healthcare. \nWhile future possibilities of implementing AI are incredibly exciting, big data in healthcare has unique challenges. Laws and regulations regarding patient privacy rights would need to be amended in order to allow patient data to be utilized as an asset. Policies that intend to protect patient information must guide data collection, data transformation, data modeling and knowledge creation. Furthermore, infrastructural changes to electronic health records and digital tools are needed in order to establish consistent data entry practices among healthcare providers and clinics (in order to minimize human error). \nMORE FOR YOU\nDeveloping Coherent AI Infrastructure For Smart Cities\nIn addition to the unique challenges in healthcare, big data also struggles with the same challenges when integrating into other industries. The major challenge in several fields has changed from finding ways of gathering data to understanding how to effectively interpret and leverage the data. Advancements in computational biology are essential to be able to store the datasets of potentially every human who has access to modern-day healthcare. In addition to the complexity of managing and storing this data, computers that are capable of doing so are incredibly expensive and space-consuming. \nIncredible discoveries in the science community have made data collection of individual health measures simple, from everyday wearables that measure vitals to mouth swabs that map out a genome. The simplicity of collecting data has shifted the focus from gathering patient information to establishing clear directions for the management, storage and interpretation of this information. Efforts made to streamline the usage of this data could have massive economical impacts on the healthcare system, improve the efficiency of healthcare teams and considerably improve patient health outcomes. \nForbes Innovation\nNutanix Rajiv Ramaswami On\nHis First Year As CEO\nForbes Technology Council is an invitation-only community for world-class CIOs, CTOs and technology executives. Do I qualify?", 'Alexey Shliakhouski is the CTO of Elinext.\nGETTY\nCovid-19 has turned our lives upside down. But it has also been a catalyst for plenty of new technology, and it made apps even more popular than they were pre-pandemic. Among the most controversial novelties are apps for tracking and preventing the spread of the virus.\nMany governments have released such apps. While the goal has been to automate tasks critical to containing the spread of Covid-19, many accuse the apps of being intrusive, discriminative, unsafe and ineffective. Let’s look at three examples to see to what extent this is true. Can Covid prevention apps be an example of good-for-society big data implementation? Or are they just another expensive yet ineffective attempt to defeat the disease?\nPROMOTED\nChina\nChina was the first country to introduce a contact tracing app into the lives of its citizens. The app Health Code was developed by the giants of the Chinese Internet: Alibaba and Tencent. The app assigns a green, yellow or red code to each citizen, which indicates their risk of having been exposed to the virus. A green code grants freedom, a yellow one means you’ll probably have to stay home for a couple of days, and red means a two-week quarantine. The criteria for color are unclear: neither the Chinese government nor the developers have revealed how the algorithm works. This, of course, has caused nationwide anxiety, scandals and distrust.\nWhile the app is not technically mandatory, it’s almost impossible to survive in China without it: Subways, malls and other public spaces require the app ― and the green code. The installation is easy: Health Code was integrated into the widely popular Alipay (online payment platform) and WeСhat (social media platform). In most cases, users didn’t have to download anything new: just provide their name, phone number, national ID number, home address, health records and travel history. Once this information is out there, it’s not private anymore: A New York Times report revealed that the app shares the information with the police.\nMORE FOR YOU\nGoogle Issues Warning For 2 Billion Chrome Users\nForget The MacBook Pro, Apple Has Bigger Plans\nGoogle Discounts Pixel 6, Nest & Pixel Buds In Limited-Time Sale Event\nThis, however, isn’t even the only concern regarding Health Code. Despite the overwhelming adoption of the app (something other countries have been struggling with), it’s impossible to assess its efficacy. There is no information on the technology behind it ― we don’t know anything about the quality of data and the hypothesis behind the algorithm. And, because the app doesn’t detect actual occurrences of close contact, there is no way to prove or disprove that it works to stop the spread of the virus.\nIndia\nIndia released its app Aarogya Setu ("bridge to health" in Sanskrit) in April 2020. Aarogya Setu is a "contact tracing, syndromic mapping, and self-assessment" app developed by the National Informatics Centre under the Ministry of Electronics and Information Technology. The Indian government made it compulsory for government and private sector employees to download it, boosting the user number enough to reach more than 100 million installs in 40 days. Some suburbs also made the app compulsory, threatening a six-month jail sentence for those who won’t comply.\nForbes Innovation\nNutanix Rajiv Ramaswami On\nHis First Year As CEO\nThe app uses Bluetooth, location data and a database of known cases of infection to notify the user if they have been close to an infected person. As Abhishek Singh, CEO of MyGov at India\'s IT ministry, told the BBC: "If you\'ve met someone in the last two weeks who has tested positive, the app calculates your risk of infection based on how recent it was and proximity, and recommends measures." It has been reported to have traced 8,436,524 suspected contacts in 322 days.\nIn terms of privacy, the app is a questionable choice, to say the least. All information collected is shared with the government. The app collects information such as the user’s name, phone number, gender, travel history and whether the user is a smoker. The code is not open source, so it can’t be audited by independent coders and researchers. Its efficacy is also not clear: Bluetooth technology leaves room for false positives. For example, as Bluetooth travels between walls and people don’t, the contact can be detected while in reality, it has never happened.\nGermany\nGermany released its CTA app much later than some other countries ― in mid-June 2020. The developers made sure it’s private, secure and non-discriminatory. If someone has a Corona-Warn-App and has been infected (has a positive Covid result from a lab and a QR code to show for it), they can then scan the code and the app will send a warning to all app users who have been near the infected person for a period of at least 15 minutes within the last 14 days. The ones who got the warning are entitled to a free Covid-19 test and are recommended to self-isolate.\nAll contacts are stored in the form of an anonymous, randomly generated ID, ensuring the users’ privacy. Neither the developers nor any third party has access to the data. No central server collects information about personal identities or locations ― all data is encrypted and saved only on users’ smartphones. The developers have also made the app\'s source code public.\nHowever, while the criteria for privacy and security are met, this simultaneously makes it hard to assess the app’s efficacy, as no one has the data. So far, the observations are not overly optimistic. As Gert G. Wagner, a member of the independent Council of Consumer Affairs Experts, which advises Germany\'s Justice Ministry, told Deutsche Welle, "Unless at least 50% of the population uses the app, only a fraction of new coronavirus infections will be detected." At the moment, Corona-Warn-App has been downloaded 34 million times, which is about 40% of the German population.\nFinal Words\nContact tracing apps act as another small step that reduces the number of people infected. And even if the number is small, it is still important. However, the efficacy of such apps could’ve been much more impressive initially, if the logic behind them and their value were communicated to people properly and if the privacy concerns were met. At the moment, the outcome of contact tracing apps remains unclear.\nForbes Technology Council is an invitation-only community for world-class CIOs, CTOs and technology executives. Do I qualify?', 'Chief Investment Officer, Defiance ETFS\nGETTY\nBig data describes the large amounts of information that could be analyzed or unanalyzed, structured or unstructured and an untapped resource or a potential headache for the whole range of businesses and sectors. Big data is too large and complex to be handled or manipulated by traditional data processing measures. It often requires the use of a supercomputer, complex algorithms, machine learning or artificial intelligence (AI) to be made practical for use. When analyzed, the data can reveal helpful patterns, trends or probabilities on topics that range from government defense, shopping behavior to medical outcomes. What is uniquely interesting about big data is that, if efficiently conquered, it has the potential to change the way that nearly all sectors and businesses operate. For this reason, I believe it is also a wonderful topic and potential investment opportunity to consider.\nThere are unique possibilities to bring efficiency to various business areas and market sectors through the analysis of big data. Here are some examples:\n• Tracking consumer traffic patterns in a brick-and-mortar store or spending and shopping search habits online and linking that information to purchase patterns can help a retailer better target the end consumer.\nPROMOTED\n• Tracking online activity and searches and combining common searches for items of clothing, cars, vacations, consumer staples and other general spending habits to support targeted ads to consumers may increase sales for e-commerce.\n• Monitoring traffic and commuter congestion could set the most efficient train and bus schedules.\n• Collecting data on various symptoms and side effects of patients allows doctors to access probabilities of various outcomes and treatment regimens based on this information\nMORE FOR YOU\nThe Top 5 Trends In Fintech And Banking For 2022\nDecade Of Investment In Big Data And AI Yield Mixed Results\nAI 50 2021: America’s Most Promising Artificial Intelligence Companies\n• Gathering data and intelligence from government agencies can help formulate specific defense strategies.\n• Financial services firms can analyze data to determine the potential risk of an investment and better plan their use of balance sheet or customer exposure. They can also use big data to create more efficient forecasting and modeling for future investments.\nWhat exactly constitutes big data? Big data can come from many sources. It is years of information stored in consumer databases on past activity. It comes from documents, emails, driving records, medical records, internet history, social media activity, computer logs and mobile devices, to name a few. Other sources of big data come from collected research or data compiled by companies and data scientists. For instance, many doctors may manually enter various statistics on a patient. Researchers can then take that data and analyze it via AI, which, in this context, is essentially mathematical models designed to find specific patterns that could be helpful to a future diagnosis.\nForbes Small Business\nHow Entrepreneurs Can\nLeverage Visualization: A\nNeuroscientist Explains\nAs an example, based on the analysis of big data, a cardiologist could analyze the outcomes of various heart surgeries to discover new remedies and procedures or to avoid certain complications liked to post-operative care strategy. This requires the use of nationwide databases and studies on millions of health records, which cannot be analyzed manually and require the use of advanced AI services. Massive volumes of data may be stored by retailers, governments and banks waiting to be analyzed and put to work. Sentiment analysis, marketing analytics and competitor studies are additional examples. The main themes in this type of data are that they are large in size, nearly impossible to manage manually and are often unstructured.\nNow that we have discussed what big data is, and why it can be beneficial to various types of businesses, we must think about how it can be cleaned and utilized and which companies could benefit from these developments. If you take a step back and think about how the world has advanced in terms of available information, you may notice how the sheer number of devices has led to exponential data availability for companies and the future growth in the various segments that address this have serious growth potential.\nThere are potential specific investment opportunities that include companies who are pioneers in the cloud, SaaS, AI, IoT, data storage and analytics. Take MicroStrategy, for example, which is a company that specializes in business intelligence and essential support for formatting reports, ad hoc queries and automating various data into reports and dashboards. Companies like Cloudera describe their business ventures as delivering an enterprise data cloud or converting any data from anywhere using AI. Another popular newcomer to the (public market) space is Snowflake, a cloud computing data warehouse company. Palantir tech is a software company that specializes in analyzing “big data.” The aforementioned companies have clientele who include everyone from the U.S. Army, CIA, government, IBM, Amazon, retailers, insurance companies, banks and hospitals. As you can see, nearly any business has relevant use of data analysis services.\nThe growth prospects for big data are also impressive. Advanced analytics tools can extract value from data and generate new business insights. In 2018, the global big data/business analytics market was valued at nearly $169 billion and is expected to increase to approximately $274 billion in 2022. In November 2018, 45% of professionals in the market research industry used big data analytics. Expert market research suggests that the market will grow to $450 billion by 2026.\nFor investors looking to gain access to this category, I recommend reviewing companies in the space of software, storage, cloud and machine learning, AI and advanced analytics as they relate to big data. Another alternative is to seek investment tools like ETFs, which directly provide a basket of securities with access to the big data space. (Full disclosure, my company offers this service, as do others.)\nIt might be easy to imagine a future where companies that specialize in or work with converting big data into a usable resource may benefit in terms of revenue generation, but all factors must be considered. The investment recipe for big data can be compelling: need-based services, a growing statistical use case and a relatively nascent market.\nThe information provided here is not investment or financial advice. You should consult with a licensed professional for advice concerning your specific situation.\nForbes Business Council is the foremost growth and networking organization for business owners and leaders. Do I qualify?', "William Niles is the Chief Executive Officer for Brinks Home.\nGETTY\nNo matter what industry you work in, by now, you have probably heard people talking about “big data.” You don’t have to be Facebook or Google to know that customer interactions can yield a wealth of information that helps you tailor, improve and enhance the customer experience. With more data available than ever before — in fact, some experts say that by 2025 there will be 463 exabytes of data created daily — it's easy to lose the human element as you measure new KPIs or NPVs on your company or customers.\nOver the last eight months, my team and I have worked on building our enterprise data hub (aka the EDH). Our organization includes numerous lines of business, each with its own unique functions, goals, expertise, challenges and data. Each domain plays a key role in the company’s goal of creating profitable accounts at scale and retaining them for life.\nPROMOTED\nA key enabler of this goal, notwithstanding amassing a talented team, is the thoughtful execution of decision science leveraging tools such as our EDH. Decision science, for us, is the use of governance and quantitative techniques applied to data that spans our people, processes and systems used to inform the decisions we make.\nNo longer is it good enough to go with your gut. The EDH serves as a secure cloud-based repository for all information generated across the enterprise. Once disparate in nature, our data sources are now pooled together with data governance applied to create a unified source of truth that can be leveraged to make more informed, data-driven decisions. This has introduced new levels of empowerment, velocity and accuracy of reporting and insight generation that help propel our business forward and improve our customer experience.\nMORE FOR YOU\nHow Gov. Newsom Can Use Texas’s Abortion Law To Restrict Guns In California\n7 Future Of Work Predictions For 2022\nSequoia Isn’t The First Hybrid, A Deep Tech Fund Raises $101,010,101 And A Startup Tackles Learning Disability Testing\nIf you’re considering using data to deliver the best experience to employees and customers alike, here are a few things we’ve learned along the way — because we know that data is nothing without people at its core.\nCreate a data hub or center.\nPulling all your information into one place breaks down data silos and creates a transparent, enterprise-wide data layer that gives visibility to all departments. This also allows you to leverage decision science to make more informed choices for customers, employees and, ultimately, your business. In our case, the hub operates under a set of governing standards that help ensure compliance with all data regulations; it also allows us to have unbiased, defined and objective information available in near real time.\nForbes Small Business\nHow Entrepreneurs Can\nLeverage Visualization: A\nNeuroscientist Explains\nReach customers more effectively through data.\nWhen you establish a robust data platform and begin applying advanced analytic/data science approaches to customer-centric use cases such as voice of customer (VoC), you begin to gain access (in real time) to all of your information streams, allowing you to identify customer sentiment around specific topics. This, in turn, allows you to bring your customer feedback and voice directly into the decision-making process for product and service innovations. An additional benefit, in our case, is that the hub has allowed us to add artificial intelligence (AI) into our customer experience, enabling our ability to offer a personalized, relevant experience to our customers based on what we’ve learned from data.\nImprove your talent recruitment and retention.\nAccess to cross-functional data helps you drive improvements to the hiring funnel, getting candidates through the process and into their new roles more efficiently. This has helped us decrease hiring time, remove hiccups in the process, decrease costs and, most importantly, improve the candidate’s as well as the company’s experience.\nThese programs are just the tip of the iceberg in terms of what we hope to accomplish with big data and growing our organization’s digital and analytic maturity models. However, the key component in each of them is remembering that the data we're mining is being used by team members to drive important decisions and is tied to the individual people we serve. As long as we continue to put people at the forefront (employees and customers alike) and enable them to make informed decisions with curated information, the future is bright.\nForbes Business Council is the foremost growth and networking organization for business owners and leaders. Do I qualify?", 'Bringing a new product to market is a difficult process. Thirty thousand new packaged consumer products are launched every year, and 95% of them fail.\nNot only do you need a fantastic idea and a USP, but you also need to make lots of other decisions – about design, manufacturing, marketing, logistics, and any number of other processes. In order to be successful, you have to get them all right – plenty of great ideas never became successful products due to mistakes made in the process of making them a reality!\nSama Tea: Using Artificial Intelligence (AI) To Launch A New Brand ADOBE STOCK\nBut what if you could use artificial intelligence (AI) to help you make those critical decisions? By using data and machine learning to inform every decision, from product development to sales and marketing, less is left to chance or "gut instinct."\nThis was the thinking adopted by one powerhouse couple – NYT best-selling author and life coach Jay Shetty and his wife, Ayurvedic chef Radhi Devlukia-Shetty, when they launched their new tea brand, Sama, in September this year.\n Sama – meaning “fully balanced” in Sanskrit, is marketed as a lifestyle drink – designed to fit the lives of the influencer couple’s audience, who follow their interest in vegan and Ayurvedic lifestyles, self-improvement, mindfulness, and wellbeing. The blends have names like "Awaken and Energise," "Protect and Support," and "Calm and Relax." And the illustration and design work that accompanies them shows serene imagery capturing scenes of togetherness and relaxation.\nMORE FOR YOU\nAI 50 2022 Nominations: Is Your Company Transforming An Industry Using Artificial Intelligence? Apply Now\nMeet The African Green Entrepreneurs Showing The West How It’s Done\nAudiobooks – An Under-Served Market For Artificial Intelligence Voice Text And Voice Solutions\nNone of this is by accident or simple gut feeling that it would be the right way to appeal to their customer base. The couple worked hand-in-hand with AI branding experts 100.co to ensure every decision was based on data and as little as possible was left to chance. The thinking was that this would be the best way to make sure Sama would be among the 5% of product launches that go on to be a success.\n100.co co-founder Kim Perell told me, "They had a vision for something special to them, something very authentic which we think is important for any brand.\n“From there, we used AI to help better understand the market, the data signals across that market, across social, retail … and understand where we can use data to differentiate our product … in terms of flavor, the packaging … that\'s what\'s really been an interesting opportunity for us."\nForbes Innovation\nNutanix Rajiv Ramaswami On\nHis First Year As CEO\nPart of what made the challenge so interesting was the need to take a data-driven approach while still creating a product that\'s "authentic" – in-line with Jay and Radhi’s vision of what they wanted to create, as well as their own unique personalities.\nFor example, while the couple initially started out by thinking about flavors, insights from the data told them that their audience was more likely to be attracted by the “adaptogenic” properties of the tea – their potential to reduce stress, invoke feelings of calmness or clarity, or offer other homeopathic benefits. Due to this, these effects were given greater prominence on the packaging and marketing materials, and the flavors themselves (which were also informed by data) were made less prominent. Virtually every other element of the finished product – including the pricing – was also informed by AI analytics carried out using 100.co’s CLAIRE platform.\nWith AI increasingly impacting more and more areas of business and our day-to-day lives, it’s likely we will see this methodology used more frequently by both start-ups and, inevitably, established producers of consumer packaged goods. Traditionally, these types of decisions would be informed by a program of focus group sessions, with products then brought to market based entirely on the data generated by a small sample of the potential customer base. Today, with social media analytics and sentiment analysis, millions of more data points can be fed into the mix after being collected and analyzed in an automated manner. The potential here is that this will lead to a far higher “hit rate” when it comes to launching successful new products. This will hopefully mean more products we actually want to buy make it to market, and at better prices, due to the efficiency gains and reduced cost.\nPhilip Smolen, 100.co’s chief platform officer, told me a little bit more about the technology behind the process. He says, “Data [and AI] are easy words to toss around … we’ve got many years of experience working with marketing intelligence platforms that only inform advertising … we really wanted to bring that intelligence further upstream into the product development process.\n“We take several major data sets, retail sales data, what’s being sold in the market, product data … not only the products available for purchase but what are the ingredients for those products, how are they packaged, how are they priced, what are their claims … then one of the key things we do that is pretty unique is tie that together with social data, what are people saying, what are the reviews, what is the sentiment?\n“Then we take toolsets like machine learning, assisted learning, natural language processing … in order to look at not just what is selling, and what is trending, but why it’s trending.”\nIt\'s an innovative approach that brings AI-powered analytics to the whole product development process. When faced with the challenge of creating and selling products for today’s digital-first, highly fragmented audiences, it’s clear new approaches to consumer engagement are needed. And the innovative strategies for doing this don’t end when the tea is sold – Sama plans to organize and host live “tea break” events, where fans are invited to join them for a virtual discussion-focused tea party via Instagram or Facebook Live – further reinforcing the idea of the product as an “experience” that fits into their customers’ lives.\nAs Devlukia-Shetty told me when we discussed it recently, “Tea allows us to slow down and have a moment to think, so the tea parties are an extension of that … helping people to have a holistic view, not just giving them tea and saying it’s going to solve your problems … it will be fun but also hopefully help people really enhance their lives.”\nYou can click here to watch my webinar with Sama co-founder Radhi Devlukia-Shetty as well as Kim Perell, Philip Smolin, and James Brennan of 100.co', 'AI Handshake GETTY\nThe business of ensuring protection from financial loss and mitigating risk is as old as human civilization. The Code of Hammurabi, King of Babylon, written in 1750-1755 B.C., specified the first provisions of what we would now know as marine insurance. In the wake of the Great Fire of London of 1666, Sir Christopher Wrenn included provisions for an “Insurance Office” in his new plan for the City of London. Today, the global insurance market is estimated to be a $5,050.31 billion industry, comprised of leading commercial life insurance, property and casualty, and health and medical insurance carriers. \nIt is against this backdrop that an emerging wave of “Insurtech” solutions companies are seeking to transform the business of insurance through the introduction of Big Data, Machine Learning, and AI capabilities. In a recent Harvard Business Review article, Legacy Companies Need to Become More Data Driven — Fast, my co-author Ash Gupta and I, observed, “In contrast to traditional insurance companies, which have been data rich but have customarily relied on actuarial approaches, startup competitors like Lemonade and Traffk are employing machine learning analytics and drawing upon thousands of data elements to provide personalized analysis and drive insurance purchases.”\nWhen insurance providers tap into the vast repositories of Big Data that is available to them and combine this data with machine learning and AI capabilities, they can develop new policies that can reach new audiences. An April 2021 report published by GlobalData forecast that AI platform revenues within insurance would grow by 23% to $3.4 billion between 2019 and 2024. It was in this context that I recently spoke with Paul Ford, Co-Founder and CEO of Traffk—one of the emerging Insurtech leaders that is employing Big Data, machine learning, and AI to help transform the insurance industry. Billing itself as a “Data Driven Insurance Underwriting and Distribution Platform”, the goal of Traffk from its inception has been to comprehend the risk and modernize the insurance underwriting process by leveraging modern data and analytics tools and technologies. \nPaul Ford knows the insurance industry firsthand, having begun his career at Farmers Insurance before moving over to Aetna as a financial underwriter. It was while working at Aetna that Ford got his first glimpse of the future of insurance, when Aetna introduced the first consumer direct health plan (HSA). From there, Ford moved to Mercer, a unit of Marsh & McLennan, where he was engaged in the use of Big Data to transform practices that resulted in better pricing of health and wellness plans. Subsequently, Ford worked at Safeway Health on solutions to manage the escalating costs of insurance premiums, all of which led up to his launch in August 2015 of Traffk.\nFord observes, “Most insurance companies don’t use a lot of data to create their products. They rely on demographic information that is 40 years old, and older. They are struggling to price policies correctly and many will miss out on huge financial opportunities because of this.” A March 2021 report from McKinsey, “Insurance 2030 – The Impact of AI on the future of insurance,” projects that “AI and its related technologies will have a seismic impact on all aspects of the insurance industry, from distribution to underwriting and pricing to claims. Advanced technologies and data are already affecting distribution and underwriting, with policies being priced, purchased, and bound in near real time.”\nMORE FOR YOU\nWill The Log4j Exploit Move Leaders To Heed Cybersecurity Warnings?\nPandemic Accelerates Need For Tech Talent With Business Expertise, Collaboration Skills\nHow To Stop Today’s Higher Education Exodus Of Women In Tech\nTraffk is, as its core, a data and analytics company that is seeking to apply Big Data, machine learning, and AI to reinvent the insurance industry. As Ford articulates it, “we are turning insurance into a large data science exercise.” This is accomplished by using the Traffk underwriting and distribution platform to leverage vast amounts of longitudinal data and 4,000 data features to optimize consumer behavior, claims behavior, signals of buying propensity, and product recommendations. The result is the delivery of offers that match “what the buyer wants to see.” Ford continues, “Insurance companies have a ton of data. Our goal is to use this data to create and design new innovative insurance products with our insurance carrier partners.”\nFord suggests that insurance companies must evolve to adapt to changing customer demographics and preferences. As an example, Ford points to millennials who are accustomed to operating in a digital world, noting, “we are repurposing old tactics for a new audience.” He credits traditional insurers, like Prudential, who recently launched their Assurance IQ solution through a 2019 acquisition. Assurance IQ provides a direct-to-consumer platform that transforms the buying experience for individuals seeking personalized health and financial wellness solutions.\nForbes Leadership\nInterview With Justin Baldoni:\nUndefining What It Means To Be\n“Man Enough” And Enacting Social Change Through Media\nDigital transformation of the insurance industry accelerated during the Covid-19 pandemic, as a growing number of consumers turned to digital channels to shop for insurance solutions. This prompted leading insurers to invigorate their digital transformation initiatives. Insurtech companies like Traffk can help traditional insurers price products more competitively, deliver products that consumers want, and improve the efficiency and convenience of the insurance purchase process for both the consumer and for the insurance agent. \nPaul Ford envisions a future where, in a digital world, those companies that succeed will be those that can quickly analyze and adapt, using data to make better business decisions and serve their customers better and faster. Ford observes, “We don’t believe that AI and machine learning will lead to a dystopian future. We are focused on incrementally using machine learning and AI to solve some very basic problems.” \nHaving lived and breathed the insurance business, Ford still maintains faith in the human elements that have made the insurance industry successful over decades and centuries. He concludes, “Insurance agents will absolutely be part of our future. They develop the client relationships and serve as a place of trust. We believe that the role of the agent will be augmented by new tools and processes. New roles may be created. The future of insurance will be a hybrid model where consumers can choose a digital or traditional experience and gracefully move between these options with ease, convenience, and efficiency.”', 'Planet satellite photo of Dukono, Indonesia. PLANET\nAs we make our way into the Thanksgiving holiday for those in the States, I had the chance to sit down with the CEO of Planet, Will Marshall, and the CEO of dMY Technology Group inc. (DMYQ) Niccolo de Masi, for an episode of the Moor Insights & Strategy Insider Podcast.\nPlanet is one of the most disruptive companies I have researched recently and one that I believe could bring a tremendous amount of value to not just one industry but to every industry.\ndMY Technology is a special acquisitions company (SPAC), and Planet will become a publicly traded company through the merger of dMY. My company Moor Insights & Strategy authored a research paper highlighting Planet’s differentiated space play. I wanted to dive deeper into Planet, asking questions on how Planet is different, its business model of big data, and other questions on Planet’s growth and future roadmap.\nWhat is Planet\nNet-net Planet is a data company. Its business model involves obtaining data from its satellites and selling it to just about anyone who could benefit from satellite imagery of the Earth. Marshall described Planet like a “Bloomberg Terminal but for Earth data” where subscribers will come in, set up their areas of interest and analytics of interest that goes directly into their workflows. I think that’s a fair comparison, and one thing I appreciate is the company’s one-to-many value proposition.\nPlanet has 200 satellites in orbit that scan the entire Earth landmass once per day. To put this accomplishment into perspective, Marshal points out that it is the largest Earth-imaging satellite constellation ever by a margin of about ten to twenty times larger than its closest competitors. In an age where data is a valuable commodity and brought many of the top companies like Google to where they are now, I think Planet is in an industry class of its own. It is even fair to say that Planet holds a monopoly on data analytics for Earth imagery because it is so many years ahead of the competition.\nMORE FROMFORBES ADVISOR\nBest Travel Insurance Companies\nByAmy DaniseEditor\nBest Covid-19 Travel Insurance Plans\nByAmy DaniseEditor\nPlanet has two fleets of orbital satellites, the Dove fleet and the SkySat fleet. PLANET\nAlthough the business model of Planet, to launch a fleet of satellites into orbit and provide Earth data to subscribers, is simple, it has multiple compounding “moats.” I love moats. Marshall described the difficulty from a hardware standpoint. Planet has to design all the different components of the satellites, including its own radio design, camera design, power system, and more. The satellites must have ground stations around the world and mission control systems. Not to mention there must be a ridiculous amount of hoops to jump through when it comes to governments and their regulations. Another moat is its proprietary software stack. Planet has built its technology stack on top of its hardware, allowing its software platforms and AI systems to scale vertically with each market that could potentially use the data.\nForbes Innovation\nNutanix Rajiv Ramaswami\nOn His First Year As CEO\nThese hardware and software “moats” compound to be what Planet calls “agile aerospace.” Marshall says that Planet has been putting new satellites into space every three months, and every time it iterates the capability and performs upgrades. Over the past year, it saw a five-fold improvement in data per day per satellite and Marshall understandably describes Planet as being on Moore’s law of constant improvements. I see Planet excelling on two fronts, the first front being as an aerospace company and the second as a data company. I believe what it has been able to achieve and improve within getting satellites into space has never been seen or done before. De Masi even pointed out that this type of technology and analytical data was reserved for governments and three-letter agencies ten years ago.\nPlanet has made this unique type of data available and valuable to multiple markets and has done so, I believe, could be five to ten years ahead of the competition.\nOne-to-many value proposition\nThis agile aerospace strategy is what allows Planet to have a one-to-many value proposition. Data analytics plays a significant role in how businesses can operate. It is fascinating that countless businesses would find value in Earth data, and it would be even more difficult to think of a company that would not find value in daily Earth data. While Planet can do the hard work of getting satellites into orbit with proprietary software, the data it acquires becomes valuable to the many. From an investment point of view, de Masi says that the more software and AI Planet builds on top of the already-valuable raw data, the more use cases Planet can serve. One subscription service of Earth data is for the many markets that would find it valuable, and the more AI and data analytics Planet can do on its end, the more markets can find the data valuable.\nThere are countless use cases and even types of data that subscribers could get from satellite imagery of Earth. Some of these large verticals include agriculture, defense and intelligence, civil, mapping and internet, forestry, energy, finance, and insurance.\nIn agriculture, Marshall explained how farmer’s fields represent about 25% of the landmass of the Earth and how Planet is the only company able to provide data analytics for a 20 to 40% improvement in crop yields. Marshall says that Planet helps farmers determine when to add fertilizer, water, and even when to harvest for each three-by-three-meter area. The efficiency that Earth data can bring to agriculture is astronomical.\nPlanet partner, FarmShots uses agricultural imagery similar this to train their automated problem ... [+] PLANET\nIn defense and intelligence, Marshall talks about how countries can see new and emerging threats that are important for peace and security. I can imagine how satellite imagery of other countries keeps those other countries accountable for their actions, not only from a diplomatic standpoint but also from a sustainability standpoint. For example, how is one country taking care of one resource that affects the welfare of another country? Similarly, Planet is useful for civil governance. Planet is able to detect buildings automatically, helping countries keep track of city planning and permit enforcement. Planet recently helped Germany when there was flooding and California with its wildfires when it came to disaster management. Satellite imagery of the Earth provides more than just geography, and as I have said before, there are countless applications to the data it provides.\nMarshall also mentioned how useful the data is to Google. Marshall says that every time something is going out of date with its Maps, Google tasks Planet’s high-resolution satellites to take pictures. Google then automatically extracts out the image and updates the map. I think of use-cases like Google Maps and know that it is just the tip of the iceberg for Google. De Massi described what Planet is doing as making the whole Earth indexed and searchable which I like because it simplifies what the company does. It is even difficult to comprehend how much value is one just one day’s worth of data. It reminds me of when 3G was available to the masses, and many people wondered what would come of all this data. Marshall mentioned that Planet is going to go to market with Google Cloud, enabling industries that are at the cross-section of these capabilities to use Planet’s data.\nAnother vertical is sustainability. What better way to realize the impact humanity is having on the Planet than to measure it and do data analytics? Marshall mentioned how he had come from a climate conference a few weeks ago and how valuable Earth data analytics is to keeping countries accountable to their sustainability commitments. Planet’s data analytics is a way to track all of humanity’s impact on the environment and a way to keep us accountable for that.\nRoadmap and growth\nWe also talked about the roadmap of Planet. Planet announced a new fleet of hyperspectral satellites that can see in 400 spectral bands (compared to the human’s ability to see across three). Marshall mentioned that this would allow Planet to detect point source methane and CO2. This sort of data is invaluable to companies like in the oil and gas industry, where they can determine gas leaks as quickly as possible. Not only that, but natural point sources of CO2 and methane are trackable. I imagine these cameras would pick up ocean currents and different types of data that are valuable to ships.\nA Planet Satellite monitoring global emissions with Hyperspectral PLANET\nMarshall also said Planet announced a new product called Fusion SAR, synthetic aperture radar (SAR) data. It combines two datasets to make a third data combination. It is being added to its product Planet Fusion.\nIf it hasn’t been made clear from our research that Planet is lightyears ahead of the competition, understand that it is not slowing down and continuing to grow. Marshall mentioned that the pipeline of Planet has grown by 45% YoY. It has also doubled its sales reps from 21, giving Planet its 100 million in revenue last year to just over 40. Another marker of growth is seeing Planet build more applications and software. Marshall said that software to go up the stack enables those vertical markets that can, in principle, get value from the data. The data needs to refine before it can truly serve in those markets.\nPlanet is going public with its merger with dMY and recently acquired VanderSat. VanderSat company provides insights to customers by analyzing satellites, with de Massi saying more acquisition announcements are coming.\nWrapping up\nThere is so much news coming out of Planet that sometimes it is difficult to keep up. Planet’s business is unique and difficult to replicate, and I believe it very well could become a household name soon. Ironically, Planet is not considered an aerospace company, yet it has the largest fleet of satellites in orbit.\nPlanet’s one-to-many value proposition allows it to scale vertically with software specific to each of its eight verticals. I believe it is in a unique position with the hardware to separate it from the competition from an aerospace perspective. From a big data perspective, it has proprietary software I believe that nobody can match. Not only this, but its one-to-many value proposition keeps innovating with each vertical. With Planet’s new fleet of hyperspectral satellites and its new Fusion SAR product, I do not see it slowing down anytime soon.\n\nNote: Moor Insights & Strategy co-op Jacob Freyman contributed to this article.', 'Co-founder and CEO, ID Quantique. 20 years of experience in R&D and management roles in optical measurements and communication systems.\nGETTY\nAs we enter year two of the quantum decade, it’s time to act now. The computing world stands on the threshold of something equal parts exciting and alarming. The age of the quantum computer is almost upon us, and it is going to radically change the way we think about big data management and security. The "Quantum Revolution" is predicted to have a greater impact than the internet on all aspects of modern life.\nData is undeniably big, but it’s not all that clever. Quantum computers will offer significant advantages when it comes to data management. The exponential growth in computing power will reap myriad benefits in the fields of statistical modeling and medical research, for example. However, the same computational power creates a clear and present threat to the public key infrastructure much of the world depends on for data security.\nPROMOTED\nIt is, perhaps, already a cliché to talk about data as the oil of the Information Age. However, there\'s no denying that in the digital economy, data is among the most precious assets of any organization because it yields opportunity. It informs all business decisions and can create a competitive advantage. But as organizations become increasingly data-rich, cybercriminals are becoming increasingly sophisticated and persistent. Our precious data resources are coming under attack.\nModern business depends on real-time access to data — anywhere, any time and on any device. Naturally, surfacing this volume of potentially sensitive data comes with challenges. Organizations need to make informed decisions about what data is available and where to ensure they provide their customers and users with the highest level of trust. At the same time, they need to balance regulatory compliance with the need for privacy and security.\nMORE FOR YOU\n27 Milestones In The History Of Quantum Computing\nQuantinuum: A New Quantum Computing Company Is Formed From Merger Of Honeywell Quantum Solutions And Cambridge Quantum\nIBM Rolls Out A Game-Changing 127-Qubit Quantum Computer That Redefines Scale, Quality, And Speed\nImpact Of A Breach\nData breaches appear to have become a fact of life. It’s rare for a week to go by without another disclosure announcing thousands, if not millions, of records have been compromised. The breach landscape is changing, though. While accidental loss or human error still accounts for over a quarter of breaches, the main protagonists are malicious outsiders.\nThe modern face of cybercrime is far more organized and well-funded than you might expect. A recent report revealed that a significant proportion of malicious activity involved hostile states exploiting vulnerabilities exacerbated by the pandemic.\nForbes Small Business\nHow Entrepreneurs Can\nLeverage Visualization: A\nNeuroscientist Explains\nThe impact of a breach may be wide-ranging. Setting aside the embarrassment factor, the average cost of a data breach last year (according to IBM) was $3.86 million. But how do we measure this cost? There are lost revenues arising from a disruption to operations and a lack of trust in the organization. There are remediation costs and potential financial penalties for a breach of regulatory compliance. There is the potential loss of proprietary data and intellectual property. In the case of negligence, there could even be criminal prosecution and imprisonment.\nIt is in the area of critical national infrastructure where the impact of a breach reaches another level. Hackers and rogue states have been increasingly targeting utility networks and public services in a high-stakes game that elevates risk to an existential level.\nIn the battle between cyberattack and defense, the defenders are frequently playing catch-up, evolving protection technologies like firewalls and anti-malware solutions to address emerging threats. With the arrival of the quantum computer, will this balance shift even more in favor of the cybercriminal, or does the quantum age offer something for the cybersecurity community, too?\nWhy Is Quantum Cryptography So Important?\nThe world’s cryptographic systems rely on complex algorithms and keys to protect data, both at rest and in motion. These codes are difficult to crack with classical computers but would pose significantly less of a challenge to a quantum computer.\nA quantum computer’s ability to solve complex problems in a fraction of the time that a traditional computer could means it will also have the power to easily crack even the strongest encryption algorithms, undermining the public key cryptography we currently rely on.\nWhile quantum computers are currently prohibitively expensive to build and run, players such as IBM and Google have demonstrated their practical applications, and it\'s likely they will become mainstream in the next 5-10 years. What can we do now to secure data and systems against the future quantum threat? The answer lies in other quantum technologies that exploit the fundamental principles of quantum physics.\nCryptographic systems have two primary functions: ensuring the confidentiality and authenticity of data. A system\'s security is determined by the strength of its encryption keys, and quantum technologies are already in use today to enhance the security of traditional encryption solutions.\nQuantum key generation is changing the way organizations secure data. As key security is dependent upon entropy (the degree of randomness used to create the keys), quantum random number generators (QRNG) are being used as a genuine source of randomness to create secure keys and help protect the authenticity and integrity of data.\nConfidentiality is addressed through quantum key distribution (QKD). QKD leverages the fundamental principle of quantum physics that observation causes perturbation. This means that if your key is intercepted (by a hacker) as it is transmitted within a communication network, you (the originator) are alerted to the fact. The corrupted key can then be discarded before it is used to encrypt data. At the end of the day, only valid secure keys are used, which ensures safe encryption and distribution of the data.\nQKD networks have been deployed worldwide to secure data for banks and financial institutions, governments, communications networks, critical infrastructure and medical organizations; my company has been building commercial QKD solutions since 2007. QRNG solutions are also in place around the world, with our QRNG chip having recently been miniaturized for use in mobile and IoT applications, including secure mobile handsets and banking apps.\nQuantum technologies are evolving at speed, with investment from state governments and global corporations helping to drive research and innovation across many sectors. One of the biggest markets for quantum computing will be the service sector, where QKD could be utilized for applications such as "cryptographic keys as a service," helping secure financial transactions and data transmission.\nAs we march toward the bright new dawn of the Fourth Industrial Revolution, the demand for big data management and cryptocurrencies will only intensify. Quantum technologies will play a major role in how we manage and secure this data in the future, with the solutions for the ultimate security authentication in the virtual world being determined by physics. As the technology landscape continues to shift, new battle lines will be drawn between cybercriminals and cybersecurity professionals. For now, it looks like quantum technologies may hand the advantage back to the good guys.\nForbes Business Council is the foremost growth and networking organization for business owners and leaders. Do I qualify?', 'The solution for an increasing number of people is Alt-tech. DEPOSIT PHOTOS\nAs a thought experiment, imagine a parent in 1985 eager to share photos of a family trip with her children. A decade before consumer-grade digital cameras became available, our hypothetical mom hands over some cash to her teens and asks them to bike over to the photo lab. So far, so good. But this is where our thought experiment takes a turn for the worse.\nOur parent has purposefully sent her kids to the bad part of town. Here, they are bombarded with sexually explicit ads, alternately cat-called and jeered by seedy residents of the neighborhood. When they at last reach the photo lab, other customers mock their pictures. They make fun of the way the kids look—even though they’ve never even met them before. To make matters worse, there’s a creep pretending to be another teen in the store and the shop owner is too busy counting money to do anything about it.\nAs observers, we would think this mom is insane for putting her children in this awful situation. It’s so obviously the wrong thing to do. So, why are most modern parents doing exactly the same thing by directing their kids to Facebook, Instagram and other social media platforms to share memories?\nSadly, the idea that Facebook and Instagram are just as bad as the neighborhood described in our thought experiment is not overblown. No one should want their children—or even adult family members—to use Mark Zuckerberg’s social media platforms for any reason, let alone share the joyful reminiscences that bring families together. To understand why, we needn’t turn to the company’s harshest critics. We can look at what Facebook’s own internal researchers are saying.\nThe Wall Street Journal has published articles detailing the nasty side of Zuckerberg’s empire in a series called “The Facebook Files.” The first features a leaked report from researchers studying both platforms’ impact on teens with a particular emphasis on girls. The findings should make your skin crawl, whether or not you have a daughter: “‘Thirty-two percent of teen girls said that when they felt bad about their bodies, Instagram made them feel worse,’ the researchers said in a March 2020 slide presentation posted to Facebook’s internal message board, reviewed by the Wall Street Journal. ‘Comparisons on Instagram can change how young women view and describe themselves.’”\nMORE FOR YOU\nPerseverance & People-First: What It Takes To Digitize A City Government\nAI 50 2022 Nominations: Is Your Company Transforming An Industry Using Artificial Intelligence? Apply Now\nCould Big Data Beat Our Opioid Crisis?\nWith a dispassionate tone, the report continues, “We make body image issues worse for one in three teen girls,” they write, before proceeding to explain how many blame the Facebook-owned photo platform for increased rates of anxiety and depression, with some teens even claiming these sites’ massive influence contributes to their suicidal feelings.\nOf course, Facebook has known about this problem for years. A more responsible company might have stepped up efforts to ensure underage users cannot access the platform and/or put in place protections to help teens overwhelmed by an increasingly toxic digital hub.\nForbes Innovation\nNutanix Rajiv Ramaswami\nOn His First Year As CEO\nFacebook didn’t choose this course, though. Instead, company decision-makers ramped up production of products aimed at even younger children. First, Facebook began working on “Instagram for Kids” for children under 13. This dubious project was only scuttled after outcry from politicians and the public, who understood just how bad of an idea it was—an obvious conclusion these executives couldn’t grasp, or more likely, chose to ignore.\nA second article by the Journal demonstrates the company’s disregard for the health and safety of its young users. The leaked documents show Facebook considers tweens the next big market. “Why do we care about tweens?” a presentation slide asks. “They are a valuable but untapped audience.” Speaking of children as young as 6 years old, a confidential 2018 document invites readers to “Imagine a Facebook experience designed for youth.”\nThis focus on younger children when the company’s platforms already possess a known negative impact on teens is incomprehensible for an ethical organization to undertake. Yet, even as Facebook chases younger users, its platforms continue to grow even more toxic for users of all ages. Additional research leaked by insiders shows the Silicon Valley giant knows exactly what celebrities and influencers make users feel bad about themselves.\nFor instance, famous people like Ariana Grande and Kylie Jenner flaunt their wealth and style in carefully constructed social media posts showing a life unobtainable for most followers. Facebook has, in effect, created several generations of users that are no longer concerned about “keeping up with the Joneses” or their own peer group. They’re instead attempting to “keep up with the Kardashians”—which is impossible for those of us that aren’t billionaires. Where teenagers once compared their clothing to the cool kids at school, now they are sizing themselves up against Instagram celebrities who don boots or t-shirts that cost as much as a car.\nAs it should be clear by now, Zuckerberg and Facebook’s executive team don’t care about any of this. Why? The people that log in to Facebook aren’t its customers, instead, people like you, me and even our hypothetical family above, are its raw material, possessing rich sources of personal data that can be sold to advertisers or other surveillance capitalists.\nAlthough we’ve focused on the company’s own research up to this point, it’s important to also consider what Zuckerberg’s critics have to say. Discussing a recently-filed securities fraud lawsuit against the company, Ohio Attorney General Dave Yost responded: “Facebook said it was looking out for our children and weeding out online trolls, but in reality was creating misery and divisiveness for profit. We are not people to Mark Zuckerberg; we are the product, and we are being used against each other out of greed.”\nOf course, some might contend that a politician in Ohio has little understanding of how Silicon Valley works. But what about Zuckerberg’s own mentor? Roger McNamee, an early investor in Facebook when it was a startup venture, has called for a criminal investigation of Zuckerberg and his leadership team.\nBy now, we’ve established that Facebook and Instagram are toxic platforms that have betrayed their promises to users, who are treated as little more than grist for the advertising mill. But this revenue model is no outlier. In fact, it extends to most of Silicon Valley, whose standard operating procedure is to create unilateral contracts of adhesion with users and then ignore all complaints (up to and including Congress).\nRecognition of this fact leaves many people reeling and powerless. They say to themselves, “I know Big Tech is bad for me and my family, but what other option do I have?” For these individuals, a day without Google, Twitter, Instagram and/or Snapchat is a day where they feel disconnected.\nBut the good news is we have alternatives. And they’re exciting, indeed.\nHow to Say, “No Thanks,” to Mark Zuckerberg and Friends\nLuckily, we aren’t expected to just grin and bear it, to hope our sons wind up with a positive view of women despite the images, videos and general trolling they encounter on a daily basis. Likewise, we needn’t pray our daughters avoid getting an eating disorder from being exposed to TikTok.\nThe solution for an increasing number of people around the world is called Alt-tech. This is a collection of companies and services designed to replace the giants of Silicon Valley with an emphasis on privacy and security. Consumers worried about Google’s snooping often opt to use DuckDuckGo for searches, while Rumble leads a growing pack of alternative videos to YouTube.\nHowever, the answer to how to share family photos without the toxicity of social media is not to utilize a different social media platform—instead, it is to use one purpose-built for sharing photos and bringing friends and loved ones closer together. A company that has successfully operated a private photo sharing platform for primarily y K-12 communities and companies is now opening it to everyone for everyday use.\n Waldo Photos built a nationwide reputation by establishing a photo sharing platform employed by organizations (like camps, schools and youth sporting events) to share pictures with parents. Now they are extending this to consumers through a reasonable monthly subscription plan.\nDesigned to remove the stress of picture sharing amongst family, friends and colleagues, so we can all get back to actually enjoying events together, Waldo Photos is completely ad-free. It also promises to avoid the toxicity of social media. Bereft of a disingenuous secondary agenda, such as profiting from Big Data snooping and selling, the company possesses one focus—the authentic sharing of photos to tighten relationship bonds. (Users interested in glimpsing celebrities pose with cars that cost more than one’s lifetime earnings will have to look elsewhere.)\nWaldo Photos CEO Rodney Rice explained to me why his company is making the shift to open up its platform for direct use: “We’ve seen the benefits it has had for the hundreds of thousands of families we’ve served through our partner communities we’ve Waldo-fied and have consistently been asked by their families if they can use Waldo for the rest of the moments in their life. Most have been looking for an alternative to social media as they know it’s bad for kids and adults, but they’ve felt a lack of alternatives heretofore. As a father of three myself, this mission is very personal for me and I’m incredibly excited by the work our team has done to provide everyone a safer, healthier alternative for sharing their photos without the toxicity, fake dopamine and never-ending popularity contest.”\nThis development couldn’t come at a more pressing time. Zuckerberg’s recent announcement that he intends to expand into the “metaverse,” by first renaming his company “Meta,” should send chills up our collective spine. As Bill Maher recently lampooned on HBO’s Real Time, such a transition will only breed more atomization, especially among so many young people who engage in the bulk of their social interactions via endless screens.\nTo this point, Rice says, “Mark Zuckerberg is pushing us towards an increasingly digital life. But that isn’t what millions, or for that matter, billions of people want. Instead, they want an authentic life based on the real-world events that become cherished memories shared by family, friends and colleagues. The Waldo Photos platform, with its easy-to-use apps, opt-in only facial recognition and automatic mobile delivery of pictures to friends and family, enables people to take the emphasis off how internet strangers may react to their pictures. Instead, it places it where it belongs: on enjoying special times together in the real world, easily sharing photos with those who have genuine interest.”\nWith its focus on authentic photo sharing and commitment to not vacuum up data and sell it to the highest bidder, Waldo Photos demonstrates the options we possess. We needn’t succumb to contracts of adhesion with labyrinthine pages of legalese just to see a pic of a loved one’s new baby. Instead, modern tech can be harnessed to improve our lives and tighten our bonds if it is built with the intention of helping society, not breaking down our mental health for profit. Along with a cast of other Alt-tech companies, Waldo Photos is proof that the future needn’t belong to Mark Zuckerberg or any of the other surveillance capitalists. It belongs to us.', "British mathematician and entrepreneur, Clive Humby coined the phrase “Data is the new oil” in 2006. Fifteen years later, the analogy still holds true. This is especially true for large businesses who often collect their ever-growing data assets into siloed data lakes within several platforms, formats and various Cloud providers, making it challenging to extract value from the data with ease for fast access and analysis.\nWhat if there were a “Google-like,” real-time, distributed query engine that could access your data no matter where it resides, in whatever form it appears in?  \nStarburst C0-Founder, Chairman and CEO, Justin Borgman. STARBURST\nThat’s the premise behind the founding of Starburst, the fast-growing, Boston-based data access and analytics company based on the Presto open-source SQL query engine originally created by four Facebook data engineers, Martin Traverso, Eric Hwang, Dain Sundstrom and David Phillips. With Presto, “You can query data in other databases, you can query data and other clouds, you can create data anywhere. And that was very unique and very novel. And so, I encouraged my co-founders to turn that into a company with me built around that technology,” says Justin Borgman, Chairman, CEO.\nFounded in 2017, Starburst began life with customers and revenue and didn’t initially seek funding. “We started a little bit unusually in that we didn't raise venture right away. We actually just sold support to existing users of the open-source project to get us off the ground and start to make some money and grow. The first couple of years we grew that way organically,” says Borgman. \nBorgman’s co-founders included Traverso, Phillips and Sundstrom as well as Matt Fuller, Kamil Bajda-Pawlikowski, Wojciech Biela, Piotr Findeisen, Anu Sudarsan, Grzegorz Kokosiński, Lukasz Osipiuk and Karol Sobczak, providing the start-up with an all-star line-up of data scientists. Hwang later joined the company reuniting the four original Presto creators. \nToday, the 300-person company has greatly expanded its offerings to serve over 150 large, enterprise customers like Comcast, Carrefour, FINRA, Verizon, and VMware to name a few.\nThey’ve attracted $164 million in venture funding from Andreessen Horowitz, Index Venture, Coatue and Salesforce Ventures, valuing the company at $1.2 billion. “Our valuations are a little crazy right now and we try not to take ourselves too seriously on that front. In fact, we like to call ourselves a workhorse, not a unicorn and have fun with with that,” says Borgman. \nMORE FOR YOU\nAI 50 2022 Nominations: Is Your Company Transforming An Industry Using Artificial Intelligence? Apply Now\nSpotify Has Plans To Move Beyond Music And Become The Instagram And TikTok Of Audio\nAI 50 2021: America’s Most Promising Artificial Intelligence Companies\nStarburst rebranded in January 2021 to avoid confusion with other Presto initiatives. The PrestoSQL query engine was itself rebranded as Trino. The Starburst name is a reference to a SQL query called “select star,” which is a search command to bring back everything. “We really liked star as a name just because it's a reference to anything and everything. The burst piece was a reference that we can retrieve anything really fast. A friend of ours came up with that. We gave him a little bit of stock for thinking about that idea and that was how we came up with the name,” says Borgman.\nThe company is Borgman’s second big-data software start-up. He founded Hadapt, the industry’s first Big Data analytic platform natively integrating SQL and Apache Hadoop in 2010. The company was acquired by Teradata in July of 2014. “I became a VP there and my job was to figure out the future of data warehousing analytics. And along the way discovered some guys at Facebook who had created a new technology for analysing data at tremendous scale with hundreds of petabytes of data and thousands of users hitting the system at the same time,” says Borgman.\nForbes Leadership\nInterview With Justin Baldoni:\nUndefining What It Means To Be\n“Man Enough” And Enacting Social Change Through Media\nBorgman was born in Chicago but grew up outside of Boston. He went to UMass Amherst for his undergrad degree and Yale for his graduate degree. “That's actually where I started my first business which was a spin out from the Yale computer science department,” says Borgman. “This is all I've ever really wanted to do. What's exciting about start-ups is it’s an opportunity to change the world in some small way, as corny as that may sound, and that's exciting.”\nHe attributes his drive to his mother’s example. “I was very lucky to have an amazing mother who, not only worked exceptionally hard, but is just a very determined, tough, driven human being. She taught me to never give up. And I really believe that start-ups are an endurance sport,” says Borgman.\nStarburst recently launched a product called Galaxy, a hosted solution that runs in the cloud. “Our current product prior to Galaxy you deploy, and you can deploy it anywhere. It could be in the cloud, it could be on prem, but you have to manage it. And so, there's kind of more work, more burden on you as a customer. Whereas Galaxy is just point and click and you've got a cluster running,” says Borgman.  \nHe believes this will broaden the market for Starburst substantially. “I think one of our challenges up to this point is that you have to be pretty smart and sophisticated to use this software. Starburst Galaxy aims to make it a lot easier so that we can go a little bit down market,” says Borgman.\nAs for the future for Starburst? \n“I want to build a big standalone independent business. [With Starburst] You don't have to move the data around. You don't have to ingest it. You don't have to get it all in one place. You can access it where it lives. And that's inherently going to give you access to more data. It gives customers access to everything that they have as a business to really draw correlations to find those hidden insights. It can be really valuable,” concludes Borgman.", 'The mainstreaming of IoT, AI and big data analytics, is providing utilities with the firepower to jettison old-world ways of working and transform into modern enterprises, note Arvind Pal Singh and David Cox, leaders in Cognizant’s Utilities practice.\nThe Work Ahead in Utilities COGNIZANT\nEven prior to the global pandemic, traditional utilities faced strong headwinds. Growing competition from digital startups on the retail side of the business threatened their market hegemony. And accelerating concerns over climate change prioritized the need to explore renewable forms of energy generation.\nBut the stakes rose to unprecedented levels by pandemic-induced disruption. As worker safety became a top priority and customer behavior changed, utilities pushed headlong to create new business models and operating environments powered by digital.\nUtilities are moving to deploy digital technologies such as the internet of things (IoT), artificial intelligence (AI) and data/analytics to automate tasks and empower frontline workers. By merging sustainability goals with digital investments, utilities are seeking transparency and a more resilient foundation for delivering measurable outcomes and winning market share. However, as processes get a digital boost, the role of humans will also evolve, requiring utilities to strike a balance by enhancing worker skillsets.\nCognizant’s Center for the Future of Work surveyed 4,000 business leaders from around the world, including 285 senior utilities executives, to understand how they are preparing for a world reshaped by the pandemic and the pivot to digital.\nProgress made in implementing IoT COGNIZANT\nCustomer Engagement - Focus of Process Augmentation COGNIZANT\nWork Done by Machines: Now and in 2023 COGNIZANT\n5 Recommendations for Creating the Utility of the Future\nBecome your own best competitor: Create digital subsidiaries that can help customers lower their utility bills using data and technologies such as smart meters.\nEmploy technology to modernize asset operations: Technologies such as IoT and analytics can help utilities modernize their networks and platforms.\nAdopt business models that go beyond utilities: Expand the role in customers’ daily lives by partnering with players outside the industry, such as insurers and broadband providers.\nDeliver on the sustainability promise with metrics: Utilities can create competitive advantage by tackling the climate crisis head-on.\nBecome new-age community leaders: Focus on enhancing customer value, serving a higher purpose, strengthening connectivity, fostering creativity, and continuous innovation.\nFor more, read “The Work Ahead in Utilities: Powering a Sustainable Future with Digital” or visit “The Work Ahead” section of our website.', 'Covid Pandemic © 2021 BLOOMBERG FINANCE LP\nLeading Fortune 1000 companies have been organizing and analyzing their data to gain business insights for years. Traditional industries with a long history of capturing and managing their data, notably financial services, have decades of experience mining data to better service and grow their customer relationships and manage risk. During the past decade, as data volumes grew, and as new sources of data emerged, Big Data initiatives captured C-suite attention. Today, Fortune 1000 companies are managing exponentially greater volumes and sources of data, while leveraging emerging technology capabilities including machine learning and AI to gain ever more granular insights into market opportunities. Companies are leveraging computers to do the work that they once relied on humans to perform.\nIt is against this backdrop that the Covid-19 plague of 2020-2021 arose, with its consequent impact on how businesses continued to serve customer needs. Many companies moved quickly to leverage digital capabilities to enable remote customer interaction and service, while data emerged into an even greater light of importance, visibility, necessity and urgency. Public and private sector organizations operating in the public health and life sciences arena faced a new urgency to manage and report on data in a timely and accurate manner. Governmental agencies, healthcare providers, universities, and industry groups mobilized to analyze vast and disparate data from around the globe to understand the velocity of spread, infection rates, risk statistics and ultimately, the path to vaccination development, rollout and development of herd immunity.\nFor healthcare and life sciences companies, this new sense of urgency and information demand is shining a spotlight on data in ways that had not been previously encountered. Data that was previously nice to have quickly became urgent and essential. This increased data urgency is reflected in the findings of NewVantage Partners annual Big Data and AI executive survey, which was published earlier this year. The survey findings point to an industry sector—life sciences and healthcare—where data has gained new preeminence and criticality. While traditionally data mature industries like financial services focused on incremental advancement of their data processes and capabilities, healthcare and life sciences firms were forced to mobilize their resources to respond quickly to public health demands. As one state governor stated during the peak of the epidemic in the spring of 2020, “We will rely on data, science, and facts” to make our resource and planning decisions.\nThe survey of C-executives from leading healthcare and life sciences companies included established leaders such as Anthem Health, Bristol-Myers Squibb, Cigna, CVS Health, Eli Lilly, Glaxo Smith Kline, Humana, Merck, Pfizer, Sanofi, and United Health, as well as emerging entrants like Cerevel. The findings dramatize the extent to which firms in the healthcare and life sciences were more aggressive, more optimistic and more successful in achieving data-driven business outcomes than their peers in other industry sectors during 2020. For example, while only 17.9% of financial services companies reported that they had created a data-driven organization, more than twice as many—40.9%—healthcare and life sciences firms reported having achieved this milestone.\nThe urgent embrace of data is reflected in other findings as well. While only 38.8% of financial services firms report that they are managing data as a critical business asset of their organization, a startling 57.1% of healthcare and life sciences firms report having achieved this outcome. These numbers are consistent down the line—66.7% of health and life sciences firms report driving innovation with data, versus 46.3% in financial services; and 45.5% of health and life science firms report having achieved transformational business outcomes, compared with 22.4% in financial services.  \nMORE FOR YOU\nBerkeley Research Lab Group Mints Second Billion-Dollar Business In Startup Anyscale\nIt’s Time For CFOs To Get Comfortable With Advanced Data Analytics\nWhy CIOs Have A Big Stake In Data Success\nThis commitment to data-driven healthcare and life sciences is further reflected in investment levels—with 50% of life science firms reporting increased investment in data, compared to 22.4% in financial services. Further, while only 40.3% of financial services firms characterized themselves as leaders in data and AI, 63.6% of life sciences and healthcare firms described themselves in this fashion. While 74.6% of financial services firms reported being focused on “offensive” initiatives, in contrast to “defensive” risk mitigation efforts, 100.0% of health and life sciences firms claimed to be on the offensive.\nFurther evidence of the commitment of life sciences companies to invest in Big Data and AI is confirmed by big pharma’s investment of billions of dollars into mining anonymized patient records to aid drug discovery. These investments are being undertaken in the hope of expanding the patient universe which will benefit from existing medicines, while also discovering new drugs. Pfizer is among the leading pharmaceutical leaders that are combing data from randomized control trials, combined with additional data sources, to create Real World Evidence (RWE). The power of data to accelerate drug discovery and reduce research costs offers new hope to patients. Developing a new drug from “bench to bedside” can cost $2.6B and take up to 14 years according to data from the Tufts University Center of Drug Development. Scientists greater understanding of individual biology is driving a search for more personalized treatments that are driven by more and better data. Today, data is being drawn from electronic health records and curated to fit research objectives. \nForbes Leadership\nInterview With Justin Baldoni:\nUndefining What It Means To Be\n“Man Enough” And Enacting Social Change Through Media\nRock Health, a US venture fund focusing on digital health, reports that $1.5B has been invested in AI or machine learning solutions that are targeted at the biopharma industry. Merck has invested $40M in TriNetX, a clinical data and analytics company, and Roivant Sciences has invested $40.5M in Datavant, which connects fragmented health data sets. RWE is being used to augment clinical trials where not enough data is available. Novartis CEO Vas Narasimhan has described his firm as “a data science company.” Novartis is looking to combine data from clinical trials with other information about patients, such as genetic make-up. Dr. Narasimhan notes, “If you look at the tech companies… they realized that actually the core asset for them is the data-mining of all of that experience.” \nThe explosion of data, and data initiatives that are underway in the life sciences has the potential to reshape the future of the industry and of patient care. This embrace of data has been coming for some time now, but the worldwide Covid pandemic generated heightened awareness and new urgency. From the application of Real-World Evidence data to the capture and analysis of new sources of genomic and epidemiological data, medical records, and other data sources, the embrace of data-driven decision making in the life sciences is gaining new momentum. Gaurav Tripathi, CTO of Innoplexus, sums up the opportunity well, “Data and machine learning will dramatically reduce the time to market for new therapies, reduce the time and investment required for targeting rare diseases, enable precise and more personalized medicines, and automate key processes that will improve efficiency by orders of magnitude.” For companies and data professionals operating in the healthcare and life sciences arena, the coming years will surely be fast paced, and will likely be transformational.']